
==> Audit <==
|--------------|--------------------------------|-----------------|-------|---------|---------------------|---------------------|
|   Command    |              Args              |     Profile     | User  | Version |     Start Time      |      End Time       |
|--------------|--------------------------------|-----------------|-------|---------|---------------------|---------------------|
| addons       | enable registry                | minikube        | praks | v1.33.0 | 21 Apr 24 23:16 IST | 21 Apr 24 23:17 IST |
| docker-env   | minikube docker-env            | minikube        | praks | v1.33.0 | 21 Apr 24 23:17 IST | 21 Apr 24 23:17 IST |
| ip           |                                | minikube        | praks | v1.33.0 | 21 Apr 24 23:17 IST | 21 Apr 24 23:17 IST |
| ip           |                                | minikube        | praks | v1.33.0 | 21 Apr 24 23:37 IST | 21 Apr 24 23:37 IST |
| docker-env   |                                | minikube        | praks | v1.33.0 | 21 Apr 24 23:37 IST | 21 Apr 24 23:37 IST |
| ip           |                                | minikube        | praks | v1.33.0 | 21 Apr 24 23:37 IST | 21 Apr 24 23:37 IST |
| docker-env   |                                | minikube        | praks | v1.33.0 | 21 Apr 24 23:40 IST |                     |
| ip           |                                | minikube        | praks | v1.33.0 | 21 Apr 24 23:40 IST |                     |
| start        |                                | minikube        | praks | v1.33.0 | 21 Apr 24 23:41 IST | 21 Apr 24 23:41 IST |
| ssh          |                                | minikube        | praks | v1.33.0 | 21 Apr 24 23:48 IST |                     |
| ssh          |                                | minikube        | praks | v1.33.0 | 21 Apr 24 23:56 IST | 22 Apr 24 00:01 IST |
| ssh          | docker pull                    | minikube        | praks | v1.33.0 | 22 Apr 24 00:20 IST |                     |
|              | blog_app-web-app:latest        |                 |       |         |                     |                     |
| ssh          | docker pull                    | minikube        | praks | v1.33.0 | 22 Apr 24 00:20 IST |                     |
|              | blog_app-web-app:latest        |                 |       |         |                     |                     |
| start        |                                | minikube        | praks | v1.33.0 | 22 Apr 24 07:29 IST | 22 Apr 24 07:29 IST |
| dashboard    | --url                          | minikube        | praks | v1.33.0 | 22 Apr 24 07:29 IST |                     |
| delete       |                                | minikube        | praks | v1.33.0 | 22 Apr 24 08:08 IST | 22 Apr 24 08:08 IST |
| start        |                                | minikube        | praks | v1.33.0 | 22 Apr 24 08:11 IST | 22 Apr 24 08:11 IST |
| node         | list                           | minikube        | praks | v1.33.0 | 22 Apr 24 08:15 IST |                     |
| start        | --driver=docker -n 2           | minikube        | praks | v1.33.0 | 22 Apr 24 08:27 IST | 22 Apr 24 08:28 IST |
| start        | --driver=docker -n 2           | minikube        | praks | v1.33.0 | 22 Apr 24 08:28 IST | 22 Apr 24 08:29 IST |
| start        | --driver=docker -n 2           | minikube        | praks | v1.33.0 | 22 Apr 24 08:29 IST | 22 Apr 24 08:30 IST |
|              | --container-runtime=cri-o      |                 |       |         |                     |                     |
| update-check |                                | minikube        | praks | v1.33.0 | 24 Apr 24 13:30 IST | 24 Apr 24 13:30 IST |
| start        |                                | minikube        | praks | v1.33.0 | 24 Apr 24 13:34 IST | 24 Apr 24 13:34 IST |
| ssh          | docker images | grep           | minikube        | praks | v1.33.0 | 24 Apr 24 14:03 IST |                     |
|              | blog_app_web-app               |                 |       |         |                     |                     |
| ssh          | docker images | grep           | minikube        | praks | v1.33.0 | 24 Apr 24 14:03 IST |                     |
|              | blog_app_web-app:latest        |                 |       |         |                     |                     |
| image        | load blog_app-web-app:latest   | minikube        | praks | v1.33.0 | 24 Apr 24 14:05 IST | 24 Apr 24 14:05 IST |
| image        | load blog_app-blog-web:latest  | minikube        | praks | v1.33.0 | 24 Apr 24 14:09 IST | 24 Apr 24 14:10 IST |
| stop         |                                | minikube        | praks | v1.33.0 | 24 Apr 24 14:38 IST | 24 Apr 24 14:38 IST |
| start        |                                | minikube        | praks | v1.33.0 | 24 Apr 24 14:39 IST | 24 Apr 24 14:39 IST |
| delete       |                                | minikube        | praks | v1.33.0 | 24 Apr 24 14:40 IST | 24 Apr 24 14:40 IST |
| start        |                                | minikube        | praks | v1.33.0 | 24 Apr 24 14:40 IST | 24 Apr 24 14:40 IST |
| ssh          | docker images | grep           | minikube        | praks | v1.33.0 | 24 Apr 24 14:43 IST |                     |
|              | blog_app-blog-web              |                 |       |         |                     |                     |
| ssh          |                                | minikube        | praks | v1.33.0 | 24 Apr 24 14:44 IST |                     |
| docker-env   |                                | minikube        | praks | v1.33.0 | 24 Apr 24 14:49 IST | 24 Apr 24 14:49 IST |
| ip           |                                | minikube        | praks | v1.33.0 | 24 Apr 24 14:50 IST | 24 Apr 24 14:50 IST |
| ssh          |                                | minikube        | praks | v1.33.0 | 24 Apr 24 14:51 IST |                     |
| ssh          |                                | minikube        | praks | v1.33.0 | 24 Apr 24 15:16 IST | 24 Apr 24 15:17 IST |
| ssh          |                                | minikube        | praks | v1.33.0 | 24 Apr 24 15:17 IST |                     |
| stop         |                                | minikube        | praks | v1.33.0 | 24 Apr 24 15:37 IST | 24 Apr 24 15:37 IST |
| delete       |                                | minikube        | praks | v1.33.0 | 24 Apr 24 15:38 IST | 24 Apr 24 15:38 IST |
| start        |                                | minikube        | praks | v1.33.0 | 24 Apr 24 15:38 IST | 24 Apr 24 15:39 IST |
| start        | --nodes 2 -p kubernetes-demo   | kubernetes-demo | praks | v1.33.0 | 25 Apr 24 11:24 IST | 25 Apr 24 11:25 IST |
| start        | -p kubernetes-demo             | kubernetes-demo | praks | v1.33.0 | 25 Apr 24 11:37 IST | 25 Apr 24 11:37 IST |
| update-check |                                | minikube        | praks | v1.33.0 | 25 Apr 24 11:43 IST | 25 Apr 24 11:43 IST |
| service      | list -p multinode-demo         | multinode-demo  | praks | v1.33.0 | 25 Apr 24 11:46 IST |                     |
| service      | list -p kubernetes-demo        | kubernetes-demo | praks | v1.33.0 | 25 Apr 24 11:46 IST | 25 Apr 24 11:46 IST |
| stop         |                                | minikube        | praks | v1.33.0 | 25 Apr 24 11:48 IST | 25 Apr 24 11:48 IST |
| stop         | -p kubernetes-demo             | kubernetes-demo | praks | v1.33.0 | 25 Apr 24 11:49 IST | 25 Apr 24 11:49 IST |
| delete       | -p kubernetes-demo             | kubernetes-demo | praks | v1.33.0 | 25 Apr 24 11:49 IST | 25 Apr 24 11:49 IST |
| update-check |                                | minikube        | praks | v1.33.0 | 25 Apr 24 11:55 IST | 25 Apr 24 11:55 IST |
| start        | -p minikube-demo               | minikube-demo   | praks | v1.33.0 | 25 Apr 24 11:56 IST | 25 Apr 24 11:57 IST |
| update-check |                                | minikube        | praks | v1.33.0 | 25 Apr 24 12:45 IST | 25 Apr 24 12:45 IST |
| ip           |                                | minikube        | praks | v1.33.0 | 25 Apr 24 13:42 IST |                     |
| stop         | -p kubernetes-demo             | kubernetes-demo | praks | v1.33.0 | 25 Apr 24 21:47 IST |                     |
| stop         | -p minikube-demo               | minikube-demo   | praks | v1.33.0 | 25 Apr 24 21:48 IST | 25 Apr 24 21:48 IST |
| delete       | -p minikube-demo               | minikube-demo   | praks | v1.33.0 | 25 Apr 24 21:48 IST | 25 Apr 24 21:48 IST |
| start        |                                | minikube        | praks | v1.33.0 | 25 Apr 24 21:48 IST | 25 Apr 24 21:49 IST |
| update-check |                                | minikube        | praks | v1.33.0 | 25 Apr 24 21:50 IST | 25 Apr 24 21:50 IST |
| ip           |                                | minikube        | praks | v1.33.0 | 25 Apr 24 21:54 IST | 25 Apr 24 21:54 IST |
| addons       | enable ingress                 | minikube        | praks | v1.33.0 | 25 Apr 24 21:58 IST |                     |
|--------------|--------------------------------|-----------------|-------|---------|---------------------|---------------------|


==> Last Start <==
Log file created at: 2024/04/25 21:48:53
Running on machine: praks
Binary: Built with gc go1.22.1 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0425 21:48:53.588286   92389 out.go:291] Setting OutFile to fd 1 ...
I0425 21:48:53.588434   92389 out.go:343] isatty.IsTerminal(1) = true
I0425 21:48:53.588439   92389 out.go:304] Setting ErrFile to fd 2...
I0425 21:48:53.588445   92389 out.go:343] isatty.IsTerminal(2) = true
I0425 21:48:53.588676   92389 root.go:338] Updating PATH: /home/praks/.minikube/bin
I0425 21:48:53.589177   92389 out.go:298] Setting JSON to false
I0425 21:48:53.590612   92389 start.go:129] hostinfo: {"hostname":"praks","uptime":45765,"bootTime":1714016169,"procs":318,"os":"linux","platform":"ubuntu","platformFamily":"debian","platformVersion":"22.04","kernelVersion":"6.5.0-28-generic","kernelArch":"x86_64","virtualizationSystem":"kvm","virtualizationRole":"host","hostId":"ab911150-be49-495c-808e-3036700610dc"}
I0425 21:48:53.590675   92389 start.go:139] virtualization: kvm host
I0425 21:48:53.594072   92389 out.go:177] üòÑ  minikube v1.33.0 on Ubuntu 22.04
I0425 21:48:53.599967   92389 notify.go:220] Checking for updates...
I0425 21:48:53.600473   92389 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.30.0
I0425 21:48:53.600579   92389 driver.go:392] Setting default libvirt URI to qemu:///system
I0425 21:48:53.629171   92389 docker.go:122] docker version: linux-26.1.0:Docker Engine - Community
I0425 21:48:53.629356   92389 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0425 21:48:53.685176   92389 info.go:266] docker info: {ID:df50da8b-c13c-4534-bb5f-3de6b145add4 Containers:4 ContainersRunning:0 ContainersPaused:0 ContainersStopped:4 Images:5 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:23 OomKillDisable:false NGoroutines:41 SystemTime:2024-04-25 21:48:53.673938224 +0530 IST LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:6.5.0-28-generic OperatingSystem:Ubuntu 22.04.4 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:12299362304 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:praks Labels:[] ExperimentalBuild:false ServerVersion:26.1.0 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:e377cd56a71523140ca6ae87e30244719194a521 Expected:e377cd56a71523140ca6ae87e30244719194a521} RuncCommit:{ID:v1.1.12-0-g51d5e94 Expected:v1.1.12-0-g51d5e94} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=apparmor name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.14.0] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.26.1]] Warnings:<nil>}}
I0425 21:48:53.685267   92389 docker.go:295] overlay module found
I0425 21:48:53.688082   92389 out.go:177] ‚ú®  Using the docker driver based on existing profile
I0425 21:48:53.693491   92389 start.go:297] selected driver: docker
I0425 21:48:53.693507   92389 start.go:901] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 Memory:2900 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:172.17.0.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/praks:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0425 21:48:53.693710   92389 start.go:912] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0425 21:48:53.693950   92389 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0425 21:48:53.763129   92389 info.go:266] docker info: {ID:df50da8b-c13c-4534-bb5f-3de6b145add4 Containers:4 ContainersRunning:0 ContainersPaused:0 ContainersStopped:4 Images:5 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:23 OomKillDisable:false NGoroutines:41 SystemTime:2024-04-25 21:48:53.7525932 +0530 IST LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:6.5.0-28-generic OperatingSystem:Ubuntu 22.04.4 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:12299362304 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:praks Labels:[] ExperimentalBuild:false ServerVersion:26.1.0 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:e377cd56a71523140ca6ae87e30244719194a521 Expected:e377cd56a71523140ca6ae87e30244719194a521} RuncCommit:{ID:v1.1.12-0-g51d5e94 Expected:v1.1.12-0-g51d5e94} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=apparmor name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.14.0] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.26.1]] Warnings:<nil>}}
I0425 21:48:53.763943   92389 cni.go:84] Creating CNI manager for ""
I0425 21:48:53.763960   92389 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0425 21:48:53.764021   92389 start.go:340] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 Memory:2900 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:172.17.0.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/praks:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0425 21:48:53.766884   92389 out.go:177] üëç  Starting "minikube" primary control-plane node in "minikube" cluster
I0425 21:48:53.772153   92389 cache.go:121] Beginning downloading kic base image for docker with docker
I0425 21:48:53.775394   92389 out.go:177] üöú  Pulling base image v0.0.43 ...
I0425 21:48:53.780820   92389 preload.go:132] Checking if preload exists for k8s version v1.30.0 and runtime docker
I0425 21:48:53.780932   92389 preload.go:147] Found local preload: /home/praks/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.30.0-docker-overlay2-amd64.tar.lz4
I0425 21:48:53.780954   92389 cache.go:56] Caching tarball of preloaded images
I0425 21:48:53.780968   92389 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 in local docker daemon
I0425 21:48:53.781304   92389 preload.go:173] Found /home/praks/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.30.0-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0425 21:48:53.781344   92389 cache.go:59] Finished verifying existence of preloaded tar for v1.30.0 on docker
I0425 21:48:53.781855   92389 profile.go:143] Saving config to /home/praks/.minikube/profiles/minikube/config.json ...
I0425 21:48:53.814279   92389 image.go:83] Found gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 in local docker daemon, skipping pull
I0425 21:48:53.814318   92389 cache.go:144] gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 exists in daemon, skipping load
I0425 21:48:53.814336   92389 cache.go:194] Successfully downloaded all kic artifacts
I0425 21:48:53.814362   92389 start.go:360] acquireMachinesLock for minikube: {Name:mkdb7605ee6259de7ebd1d49d0cb69a8281b4a77 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0425 21:48:53.814482   92389 start.go:364] duration metric: took 92.913¬µs to acquireMachinesLock for "minikube"
I0425 21:48:53.814505   92389 start.go:96] Skipping create...Using existing machine configuration
I0425 21:48:53.814511   92389 fix.go:54] fixHost starting: 
I0425 21:48:53.814986   92389 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0425 21:48:53.834271   92389 fix.go:112] recreateIfNeeded on minikube: state=Stopped err=<nil>
W0425 21:48:53.834290   92389 fix.go:138] unexpected machine state, will restart: <nil>
I0425 21:48:53.837249   92389 out.go:177] üîÑ  Restarting existing docker container for "minikube" ...
I0425 21:48:53.840033   92389 cli_runner.go:164] Run: docker start minikube
I0425 21:48:54.358308   92389 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0425 21:48:54.399133   92389 kic.go:430] container "minikube" state is running.
I0425 21:48:54.399764   92389 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0425 21:48:54.421966   92389 profile.go:143] Saving config to /home/praks/.minikube/profiles/minikube/config.json ...
I0425 21:48:54.422271   92389 machine.go:94] provisionDockerMachine start ...
I0425 21:48:54.422364   92389 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0425 21:48:54.446434   92389 main.go:141] libmachine: Using SSH client type: native
I0425 21:48:54.446836   92389 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82d4e0] 0x830240 <nil>  [] 0s} 127.0.0.1 32787 <nil> <nil>}
I0425 21:48:54.446849   92389 main.go:141] libmachine: About to run SSH command:
hostname
I0425 21:48:54.447562   92389 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: read tcp 127.0.0.1:41522->127.0.0.1:32787: read: connection reset by peer
I0425 21:48:57.635278   92389 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0425 21:48:57.635323   92389 ubuntu.go:169] provisioning hostname "minikube"
I0425 21:48:57.635559   92389 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0425 21:48:57.686378   92389 main.go:141] libmachine: Using SSH client type: native
I0425 21:48:57.686675   92389 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82d4e0] 0x830240 <nil>  [] 0s} 127.0.0.1 32787 <nil> <nil>}
I0425 21:48:57.686684   92389 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0425 21:48:57.889633   92389 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0425 21:48:57.889740   92389 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0425 21:48:57.910809   92389 main.go:141] libmachine: Using SSH client type: native
I0425 21:48:57.911031   92389 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82d4e0] 0x830240 <nil>  [] 0s} 127.0.0.1 32787 <nil> <nil>}
I0425 21:48:57.911067   92389 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0425 21:48:58.057204   92389 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0425 21:48:58.057222   92389 ubuntu.go:175] set auth options {CertDir:/home/praks/.minikube CaCertPath:/home/praks/.minikube/certs/ca.pem CaPrivateKeyPath:/home/praks/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/praks/.minikube/machines/server.pem ServerKeyPath:/home/praks/.minikube/machines/server-key.pem ClientKeyPath:/home/praks/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/praks/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/praks/.minikube}
I0425 21:48:58.057237   92389 ubuntu.go:177] setting up certificates
I0425 21:48:58.057244   92389 provision.go:84] configureAuth start
I0425 21:48:58.057327   92389 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0425 21:48:58.077935   92389 provision.go:143] copyHostCerts
I0425 21:48:58.077988   92389 exec_runner.go:144] found /home/praks/.minikube/key.pem, removing ...
I0425 21:48:58.077994   92389 exec_runner.go:203] rm: /home/praks/.minikube/key.pem
I0425 21:48:58.078062   92389 exec_runner.go:151] cp: /home/praks/.minikube/certs/key.pem --> /home/praks/.minikube/key.pem (1679 bytes)
I0425 21:48:58.084983   92389 exec_runner.go:144] found /home/praks/.minikube/ca.pem, removing ...
I0425 21:48:58.084990   92389 exec_runner.go:203] rm: /home/praks/.minikube/ca.pem
I0425 21:48:58.085034   92389 exec_runner.go:151] cp: /home/praks/.minikube/certs/ca.pem --> /home/praks/.minikube/ca.pem (1074 bytes)
I0425 21:48:58.085127   92389 exec_runner.go:144] found /home/praks/.minikube/cert.pem, removing ...
I0425 21:48:58.085132   92389 exec_runner.go:203] rm: /home/praks/.minikube/cert.pem
I0425 21:48:58.085170   92389 exec_runner.go:151] cp: /home/praks/.minikube/certs/cert.pem --> /home/praks/.minikube/cert.pem (1119 bytes)
I0425 21:48:58.085242   92389 provision.go:117] generating server cert: /home/praks/.minikube/machines/server.pem ca-key=/home/praks/.minikube/certs/ca.pem private-key=/home/praks/.minikube/certs/ca-key.pem org=praks.minikube san=[127.0.0.1 172.17.0.2 localhost minikube]
I0425 21:48:58.265656   92389 provision.go:177] copyRemoteCerts
I0425 21:48:58.265708   92389 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0425 21:48:58.265749   92389 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0425 21:48:58.285509   92389 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32787 SSHKeyPath:/home/praks/.minikube/machines/minikube/id_rsa Username:docker}
I0425 21:48:58.413561   92389 ssh_runner.go:362] scp /home/praks/.minikube/machines/server.pem --> /etc/docker/server.pem (1176 bytes)
I0425 21:48:58.458247   92389 ssh_runner.go:362] scp /home/praks/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I0425 21:48:58.495471   92389 ssh_runner.go:362] scp /home/praks/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1074 bytes)
I0425 21:48:58.531790   92389 provision.go:87] duration metric: took 474.534662ms to configureAuth
I0425 21:48:58.531811   92389 ubuntu.go:193] setting minikube options for container-runtime
I0425 21:48:58.531966   92389 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.30.0
I0425 21:48:58.532015   92389 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0425 21:48:58.552346   92389 main.go:141] libmachine: Using SSH client type: native
I0425 21:48:58.552543   92389 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82d4e0] 0x830240 <nil>  [] 0s} 127.0.0.1 32787 <nil> <nil>}
I0425 21:48:58.552551   92389 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0425 21:48:58.725023   92389 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0425 21:48:58.725063   92389 ubuntu.go:71] root file system type: overlay
I0425 21:48:58.725583   92389 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0425 21:48:58.725818   92389 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0425 21:48:58.765413   92389 main.go:141] libmachine: Using SSH client type: native
I0425 21:48:58.765643   92389 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82d4e0] 0x830240 <nil>  [] 0s} 127.0.0.1 32787 <nil> <nil>}
I0425 21:48:58.765895   92389 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0425 21:48:58.952469   92389 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0425 21:48:58.952550   92389 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0425 21:48:58.972883   92389 main.go:141] libmachine: Using SSH client type: native
I0425 21:48:58.973110   92389 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82d4e0] 0x830240 <nil>  [] 0s} 127.0.0.1 32787 <nil> <nil>}
I0425 21:48:58.973150   92389 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0425 21:48:59.147395   92389 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0425 21:48:59.147439   92389 machine.go:97] duration metric: took 4.725148501s to provisionDockerMachine
I0425 21:48:59.147472   92389 start.go:293] postStartSetup for "minikube" (driver="docker")
I0425 21:48:59.147507   92389 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0425 21:48:59.147749   92389 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0425 21:48:59.147964   92389 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0425 21:48:59.178105   92389 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32787 SSHKeyPath:/home/praks/.minikube/machines/minikube/id_rsa Username:docker}
I0425 21:48:59.315122   92389 ssh_runner.go:195] Run: cat /etc/os-release
I0425 21:48:59.320753   92389 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0425 21:48:59.320811   92389 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0425 21:48:59.320833   92389 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0425 21:48:59.320843   92389 info.go:137] Remote host: Ubuntu 22.04.4 LTS
I0425 21:48:59.320859   92389 filesync.go:126] Scanning /home/praks/.minikube/addons for local assets ...
I0425 21:48:59.320956   92389 filesync.go:126] Scanning /home/praks/.minikube/files for local assets ...
I0425 21:48:59.321001   92389 start.go:296] duration metric: took 173.517923ms for postStartSetup
I0425 21:48:59.321109   92389 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0425 21:48:59.321179   92389 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0425 21:48:59.344319   92389 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32787 SSHKeyPath:/home/praks/.minikube/machines/minikube/id_rsa Username:docker}
I0425 21:48:59.455976   92389 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0425 21:48:59.474072   92389 fix.go:56] duration metric: took 5.659546668s for fixHost
I0425 21:48:59.474100   92389 start.go:83] releasing machines lock for "minikube", held for 5.65960339s
I0425 21:48:59.474269   92389 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0425 21:48:59.499751   92389 ssh_runner.go:195] Run: cat /version.json
I0425 21:48:59.499779   92389 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0425 21:48:59.499814   92389 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0425 21:48:59.499840   92389 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0425 21:48:59.523038   92389 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32787 SSHKeyPath:/home/praks/.minikube/machines/minikube/id_rsa Username:docker}
I0425 21:48:59.525647   92389 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32787 SSHKeyPath:/home/praks/.minikube/machines/minikube/id_rsa Username:docker}
I0425 21:48:59.621471   92389 ssh_runner.go:195] Run: systemctl --version
I0425 21:49:01.666192   92389 ssh_runner.go:235] Completed: curl -sS -m 2 https://registry.k8s.io/: (2.166331416s)
I0425 21:49:01.666308   92389 ssh_runner.go:235] Completed: systemctl --version: (2.04475585s)
W0425 21:49:01.666382   92389 start.go:860] [curl -sS -m 2 https://registry.k8s.io/] failed: curl -sS -m 2 https://registry.k8s.io/: Process exited with status 28
stdout:

stderr:
curl: (28) Operation timed out after 2000 milliseconds with 0 bytes received
I0425 21:49:01.666558   92389 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
W0425 21:49:01.666600   92389 out.go:239] ‚ùó  This container is having trouble accessing https://registry.k8s.io
W0425 21:49:01.666727   92389 out.go:239] üí°  To pull new external images, you may need to configure a proxy: https://minikube.sigs.k8s.io/docs/reference/networking/proxy/
I0425 21:49:01.679622   92389 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I0425 21:49:01.708516   92389 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I0425 21:49:01.708602   92389 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0425 21:49:01.722466   92389 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I0425 21:49:01.722481   92389 start.go:494] detecting cgroup driver to use...
I0425 21:49:01.722510   92389 detect.go:199] detected "systemd" cgroup driver on host os
I0425 21:49:01.722596   92389 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0425 21:49:01.746539   92389 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I0425 21:49:01.760954   92389 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0425 21:49:01.775339   92389 containerd.go:146] configuring containerd to use "systemd" as cgroup driver...
I0425 21:49:01.775416   92389 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = true|g' /etc/containerd/config.toml"
I0425 21:49:01.789528   92389 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0425 21:49:01.803365   92389 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0425 21:49:01.817205   92389 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0425 21:49:01.831128   92389 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0425 21:49:01.844901   92389 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0425 21:49:01.858917   92389 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0425 21:49:01.873050   92389 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0425 21:49:01.887290   92389 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0425 21:49:01.900336   92389 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0425 21:49:01.913580   92389 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0425 21:49:02.100268   92389 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0425 21:49:02.246480   92389 start.go:494] detecting cgroup driver to use...
I0425 21:49:02.246516   92389 detect.go:199] detected "systemd" cgroup driver on host os
I0425 21:49:02.246599   92389 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0425 21:49:02.278418   92389 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0425 21:49:02.278498   92389 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0425 21:49:02.305441   92389 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0425 21:49:02.343401   92389 ssh_runner.go:195] Run: which cri-dockerd
I0425 21:49:02.351284   92389 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0425 21:49:02.374089   92389 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (189 bytes)
I0425 21:49:02.425681   92389 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0425 21:49:02.663543   92389 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0425 21:49:02.840511   92389 docker.go:574] configuring docker to use "systemd" as cgroup driver...
I0425 21:49:02.840625   92389 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (129 bytes)
I0425 21:49:02.868487   92389 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0425 21:49:03.041948   92389 ssh_runner.go:195] Run: sudo systemctl restart docker
I0425 21:49:03.575321   92389 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0425 21:49:03.592101   92389 ssh_runner.go:195] Run: sudo systemctl stop cri-docker.socket
I0425 21:49:03.609195   92389 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0425 21:49:03.625235   92389 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0425 21:49:03.770632   92389 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0425 21:49:03.897911   92389 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0425 21:49:04.020939   92389 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0425 21:49:04.075418   92389 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0425 21:49:04.093219   92389 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0425 21:49:04.200565   92389 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0425 21:49:04.346198   92389 start.go:541] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0425 21:49:04.346301   92389 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0425 21:49:04.351917   92389 start.go:562] Will wait 60s for crictl version
I0425 21:49:04.351995   92389 ssh_runner.go:195] Run: which crictl
I0425 21:49:04.356951   92389 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0425 21:49:04.447748   92389 start.go:578] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  26.0.1
RuntimeApiVersion:  v1
I0425 21:49:04.447808   92389 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0425 21:49:04.477303   92389 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0425 21:49:04.509008   92389 out.go:204] üê≥  Preparing Kubernetes v1.30.0 on Docker 26.0.1 ...
I0425 21:49:04.509114   92389 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
W0425 21:49:04.528691   92389 cli_runner.go:211] docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}" returned with exit code 1
I0425 21:49:04.528778   92389 network_create.go:281] running [docker network inspect minikube] to gather additional debugging logs...
I0425 21:49:04.528802   92389 cli_runner.go:164] Run: docker network inspect minikube
W0425 21:49:04.548267   92389 cli_runner.go:211] docker network inspect minikube returned with exit code 1
I0425 21:49:04.548288   92389 network_create.go:284] error running [docker network inspect minikube]: docker network inspect minikube: exit status 1
stdout:
[]

stderr:
Error response from daemon: network minikube not found
I0425 21:49:04.548299   92389 network_create.go:286] output of [docker network inspect minikube]: -- stdout --
[]

-- /stdout --
** stderr ** 
Error response from daemon: network minikube not found

** /stderr **
I0425 21:49:04.548350   92389 network.go:69] The container minikube is not attached to a network, this could be because the cluster was created by minikube <v1.14, will try to get the IP using container gateway
I0425 21:49:04.548404   92389 cli_runner.go:164] Run: docker container inspect --format {{.NetworkSettings.Gateway}} minikube
I0425 21:49:04.568227   92389 ssh_runner.go:195] Run: grep 172.17.0.1	host.minikube.internal$ /etc/hosts
I0425 21:49:04.573230   92389 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "172.17.0.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0425 21:49:04.589403   92389 kubeadm.go:877] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 Memory:2900 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:172.17.0.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/praks:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0425 21:49:04.589481   92389 preload.go:132] Checking if preload exists for k8s version v1.30.0 and runtime docker
I0425 21:49:04.589546   92389 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0425 21:49:04.615057   92389 docker.go:685] Got preloaded images: -- stdout --
blog-web:v1
registry.k8s.io/kube-apiserver:v1.30.0
registry.k8s.io/kube-controller-manager:v1.30.0
registry.k8s.io/kube-scheduler:v1.30.0
registry.k8s.io/kube-proxy:v1.30.0
postgres:latest
registry.k8s.io/etcd:3.5.12-0
registry.k8s.io/coredns/coredns:v1.11.1
registry.k8s.io/pause:3.9
postgres:14.2-alpine
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0425 21:49:04.615068   92389 docker.go:615] Images already preloaded, skipping extraction
I0425 21:49:04.615132   92389 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0425 21:49:04.640094   92389 docker.go:685] Got preloaded images: -- stdout --
blog-web:v1
registry.k8s.io/kube-apiserver:v1.30.0
registry.k8s.io/kube-controller-manager:v1.30.0
registry.k8s.io/kube-scheduler:v1.30.0
registry.k8s.io/kube-proxy:v1.30.0
postgres:latest
registry.k8s.io/etcd:3.5.12-0
registry.k8s.io/coredns/coredns:v1.11.1
registry.k8s.io/pause:3.9
postgres:14.2-alpine
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0425 21:49:04.640112   92389 cache_images.go:84] Images are preloaded, skipping loading
I0425 21:49:04.640129   92389 kubeadm.go:928] updating node { 172.17.0.2 8443 v1.30.0 docker true true} ...
I0425 21:49:04.640411   92389 kubeadm.go:940] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.30.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=172.17.0.2

[Install]
 config:
{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0425 21:49:04.640471   92389 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0425 21:49:04.774709   92389 cni.go:84] Creating CNI manager for ""
I0425 21:49:04.774730   92389 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0425 21:49:04.774745   92389 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0425 21:49:04.774776   92389 kubeadm.go:181] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:172.17.0.2 APIServerPort:8443 KubernetesVersion:v1.30.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "172.17.0.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:172.17.0.2 CgroupDriver:systemd ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0425 21:49:04.774893   92389 kubeadm.go:187] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 172.17.0.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 172.17.0.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "172.17.0.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.30.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: systemd
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0425 21:49:04.774951   92389 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.30.0
I0425 21:49:04.791510   92389 binaries.go:44] Found k8s binaries, skipping transfer
I0425 21:49:04.791651   92389 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0425 21:49:04.804873   92389 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (305 bytes)
I0425 21:49:04.831688   92389 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0425 21:49:04.859565   92389 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2143 bytes)
I0425 21:49:04.887674   92389 ssh_runner.go:195] Run: grep 172.17.0.2	control-plane.minikube.internal$ /etc/hosts
I0425 21:49:04.892150   92389 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "172.17.0.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0425 21:49:04.908350   92389 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0425 21:49:05.051556   92389 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0425 21:49:05.087380   92389 certs.go:68] Setting up /home/praks/.minikube/profiles/minikube for IP: 172.17.0.2
I0425 21:49:05.087391   92389 certs.go:194] generating shared ca certs ...
I0425 21:49:05.087406   92389 certs.go:226] acquiring lock for ca certs: {Name:mk0673cbc3946b87638bf3c4956212cd0629d141 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0425 21:49:05.087545   92389 certs.go:235] skipping valid "minikubeCA" ca cert: /home/praks/.minikube/ca.key
I0425 21:49:05.087629   92389 certs.go:235] skipping valid "proxyClientCA" ca cert: /home/praks/.minikube/proxy-client-ca.key
I0425 21:49:05.087640   92389 certs.go:256] generating profile certs ...
I0425 21:49:05.091645   92389 certs.go:359] skipping valid signed profile cert regeneration for "minikube-user": /home/praks/.minikube/profiles/minikube/client.key
I0425 21:49:05.091821   92389 certs.go:359] skipping valid signed profile cert regeneration for "minikube": /home/praks/.minikube/profiles/minikube/apiserver.key.f55e2435
I0425 21:49:05.091986   92389 certs.go:359] skipping valid signed profile cert regeneration for "aggregator": /home/praks/.minikube/profiles/minikube/proxy-client.key
I0425 21:49:05.092168   92389 certs.go:484] found cert: /home/praks/.minikube/certs/ca-key.pem (1679 bytes)
I0425 21:49:05.092202   92389 certs.go:484] found cert: /home/praks/.minikube/certs/ca.pem (1074 bytes)
I0425 21:49:05.092231   92389 certs.go:484] found cert: /home/praks/.minikube/certs/cert.pem (1119 bytes)
I0425 21:49:05.092258   92389 certs.go:484] found cert: /home/praks/.minikube/certs/key.pem (1679 bytes)
I0425 21:49:05.092917   92389 ssh_runner.go:362] scp /home/praks/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0425 21:49:05.133731   92389 ssh_runner.go:362] scp /home/praks/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I0425 21:49:05.175064   92389 ssh_runner.go:362] scp /home/praks/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0425 21:49:05.215944   92389 ssh_runner.go:362] scp /home/praks/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I0425 21:49:05.257884   92389 ssh_runner.go:362] scp /home/praks/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0425 21:49:05.300238   92389 ssh_runner.go:362] scp /home/praks/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I0425 21:49:05.363528   92389 ssh_runner.go:362] scp /home/praks/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0425 21:49:05.436840   92389 ssh_runner.go:362] scp /home/praks/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I0425 21:49:05.488096   92389 ssh_runner.go:362] scp /home/praks/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0425 21:49:05.536676   92389 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0425 21:49:05.567309   92389 ssh_runner.go:195] Run: openssl version
I0425 21:49:05.576854   92389 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0425 21:49:05.592099   92389 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0425 21:49:05.597186   92389 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Apr 21 07:06 /usr/share/ca-certificates/minikubeCA.pem
I0425 21:49:05.597257   92389 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0425 21:49:05.606560   92389 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0425 21:49:05.620343   92389 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0425 21:49:05.625561   92389 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I0425 21:49:05.634762   92389 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I0425 21:49:05.643744   92389 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I0425 21:49:05.653963   92389 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I0425 21:49:05.663478   92389 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I0425 21:49:05.673687   92389 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I0425 21:49:05.682511   92389 kubeadm.go:391] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 Memory:2900 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:172.17.0.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/praks:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0425 21:49:05.682624   92389 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0425 21:49:05.710736   92389 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
W0425 21:49:05.724823   92389 kubeadm.go:404] apiserver tunnel failed: apiserver port not set
I0425 21:49:05.724838   92389 kubeadm.go:407] found existing configuration files, will attempt cluster restart
I0425 21:49:05.724844   92389 kubeadm.go:587] restartPrimaryControlPlane start ...
I0425 21:49:05.724903   92389 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0425 21:49:05.738096   92389 kubeadm.go:129] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0425 21:49:05.738602   92389 kubeconfig.go:47] verify endpoint returned: get endpoint: "minikube" does not appear in /home/praks/.kube/config
I0425 21:49:05.738747   92389 kubeconfig.go:62] /home/praks/.kube/config needs updating (will repair): [kubeconfig missing "minikube" cluster setting kubeconfig missing "minikube" context setting]
I0425 21:49:05.739133   92389 lock.go:35] WriteFile acquiring /home/praks/.kube/config: {Name:mk1623c39f51b7834e6cbd8e88d977556167a661 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0425 21:49:05.741210   92389 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0425 21:49:05.757637   92389 kubeadm.go:624] The running cluster does not require reconfiguration: 172.17.0.2
I0425 21:49:05.757660   92389 kubeadm.go:591] duration metric: took 32.810519ms to restartPrimaryControlPlane
I0425 21:49:05.757681   92389 kubeadm.go:393] duration metric: took 75.176107ms to StartCluster
I0425 21:49:05.757701   92389 settings.go:142] acquiring lock: {Name:mked70111d23c8a0bfb5c8e74eab548780cd042b Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0425 21:49:05.757774   92389 settings.go:150] Updating kubeconfig:  /home/praks/.kube/config
I0425 21:49:05.758364   92389 lock.go:35] WriteFile acquiring /home/praks/.kube/config: {Name:mk1623c39f51b7834e6cbd8e88d977556167a661 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0425 21:49:05.758570   92389 start.go:234] Will wait 6m0s for node &{Name: IP:172.17.0.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0425 21:49:05.762015   92389 out.go:177] üîé  Verifying Kubernetes components...
I0425 21:49:05.758666   92389 addons.go:502] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volumesnapshots:false yakd:false]
I0425 21:49:05.758847   92389 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.30.0
I0425 21:49:05.762075   92389 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0425 21:49:05.762088   92389 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0425 21:49:05.765024   92389 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0425 21:49:05.765061   92389 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0425 21:49:05.765107   92389 addons.go:234] Setting addon storage-provisioner=true in "minikube"
W0425 21:49:05.765121   92389 addons.go:243] addon storage-provisioner should already be in state true
I0425 21:49:05.765151   92389 host.go:66] Checking if "minikube" exists ...
I0425 21:49:05.765506   92389 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0425 21:49:05.766067   92389 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0425 21:49:05.799190   92389 addons.go:234] Setting addon default-storageclass=true in "minikube"
W0425 21:49:05.799203   92389 addons.go:243] addon default-storageclass should already be in state true
I0425 21:49:05.799233   92389 host.go:66] Checking if "minikube" exists ...
I0425 21:49:05.800066   92389 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0425 21:49:05.812625   92389 out.go:177]     ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0425 21:49:05.815777   92389 addons.go:426] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0425 21:49:05.815797   92389 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0425 21:49:05.815895   92389 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0425 21:49:05.851014   92389 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32787 SSHKeyPath:/home/praks/.minikube/machines/minikube/id_rsa Username:docker}
I0425 21:49:05.851057   92389 addons.go:426] installing /etc/kubernetes/addons/storageclass.yaml
I0425 21:49:05.851069   92389 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0425 21:49:05.851146   92389 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0425 21:49:05.882007   92389 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32787 SSHKeyPath:/home/praks/.minikube/machines/minikube/id_rsa Username:docker}
I0425 21:49:05.983138   92389 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0425 21:49:05.991877   92389 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0425 21:49:06.036921   92389 api_server.go:52] waiting for apiserver process to appear ...
I0425 21:49:06.036996   92389 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0425 21:49:06.048207   92389 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
W0425 21:49:06.389703   92389 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0425 21:49:06.389725   92389 retry.go:31] will retry after 156.322376ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W0425 21:49:06.394738   92389 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0425 21:49:06.394759   92389 retry.go:31] will retry after 146.210465ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0425 21:49:06.537967   92389 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0425 21:49:06.541715   92389 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I0425 21:49:06.546445   92389 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W0425 21:49:06.758379   92389 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0425 21:49:06.758402   92389 retry.go:31] will retry after 463.611911ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W0425 21:49:06.798037   92389 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0425 21:49:06.798065   92389 retry.go:31] will retry after 346.981616ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0425 21:49:07.037511   92389 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0425 21:49:07.079808   92389 api_server.go:72] duration metric: took 1.320979054s to wait for apiserver process to appear ...
I0425 21:49:07.079824   92389 api_server.go:88] waiting for apiserver healthz status ...
I0425 21:49:07.079843   92389 api_server.go:253] Checking apiserver healthz at https://172.17.0.2:8443/healthz ...
I0425 21:49:07.080192   92389 api_server.go:269] stopped: https://172.17.0.2:8443/healthz: Get "https://172.17.0.2:8443/healthz": dial tcp 172.17.0.2:8443: connect: connection refused
I0425 21:49:07.145487   92389 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0425 21:49:07.223059   92389 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I0425 21:49:07.580931   92389 api_server.go:253] Checking apiserver healthz at https://172.17.0.2:8443/healthz ...
I0425 21:49:09.508048   92389 api_server.go:279] https://172.17.0.2:8443/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0425 21:49:09.508067   92389 api_server.go:103] status: https://172.17.0.2:8443/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0425 21:49:09.508083   92389 api_server.go:253] Checking apiserver healthz at https://172.17.0.2:8443/healthz ...
I0425 21:49:09.532378   92389 api_server.go:279] https://172.17.0.2:8443/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0425 21:49:09.532397   92389 api_server.go:103] status: https://172.17.0.2:8443/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0425 21:49:09.580620   92389 api_server.go:253] Checking apiserver healthz at https://172.17.0.2:8443/healthz ...
I0425 21:49:09.587518   92389 api_server.go:279] https://172.17.0.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[+]poststarthook/apiservice-status-available-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0425 21:49:09.587538   92389 api_server.go:103] status: https://172.17.0.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[+]poststarthook/apiservice-status-available-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0425 21:49:10.080014   92389 api_server.go:253] Checking apiserver healthz at https://172.17.0.2:8443/healthz ...
I0425 21:49:10.084114   92389 api_server.go:279] https://172.17.0.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0425 21:49:10.084131   92389 api_server.go:103] status: https://172.17.0.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0425 21:49:10.304686   92389 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: (3.159171452s)
I0425 21:49:10.304717   92389 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: (3.081639513s)
I0425 21:49:10.320134   92389 out.go:177] üåü  Enabled addons: storage-provisioner, default-storageclass
I0425 21:49:10.322990   92389 addons.go:505] duration metric: took 4.564325315s for enable addons: enabled=[storage-provisioner default-storageclass]
I0425 21:49:10.580601   92389 api_server.go:253] Checking apiserver healthz at https://172.17.0.2:8443/healthz ...
I0425 21:49:10.585280   92389 api_server.go:279] https://172.17.0.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0425 21:49:10.585304   92389 api_server.go:103] status: https://172.17.0.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0425 21:49:11.081427   92389 api_server.go:253] Checking apiserver healthz at https://172.17.0.2:8443/healthz ...
I0425 21:49:11.089656   92389 api_server.go:279] https://172.17.0.2:8443/healthz returned 200:
ok
I0425 21:49:11.091036   92389 api_server.go:141] control plane version: v1.30.0
I0425 21:49:11.091057   92389 api_server.go:131] duration metric: took 4.011225659s to wait for apiserver health ...
I0425 21:49:11.091065   92389 system_pods.go:43] waiting for kube-system pods to appear ...
I0425 21:49:11.106368   92389 system_pods.go:59] 7 kube-system pods found
I0425 21:49:11.106403   92389 system_pods.go:61] "coredns-7db6d8ff4d-c86rh" [4984df84-69f6-4395-b150-35fcdacf9115] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0425 21:49:11.106415   92389 system_pods.go:61] "etcd-minikube" [ba0966f1-49e6-4071-906d-6947c85350b8] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0425 21:49:11.106427   92389 system_pods.go:61] "kube-apiserver-minikube" [fa9b8e24-1e7c-4ac4-b738-9dcadc0e61ca] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0425 21:49:11.106437   92389 system_pods.go:61] "kube-controller-manager-minikube" [bac24aaf-99f4-40d2-843b-28e91ef15e9d] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0425 21:49:11.106446   92389 system_pods.go:61] "kube-proxy-smrmv" [50a27a53-ad03-4f24-9a49-6f0830b11ca2] Running / Ready:ContainersNotReady (containers with unready status: [kube-proxy]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-proxy])
I0425 21:49:11.106468   92389 system_pods.go:61] "kube-scheduler-minikube" [1c05b5b3-eb44-4541-8981-609613e96bd2] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0425 21:49:11.106478   92389 system_pods.go:61] "storage-provisioner" [fcf950ea-5106-4379-a719-0c4495952baa] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I0425 21:49:11.106486   92389 system_pods.go:74] duration metric: took 15.41406ms to wait for pod list to return data ...
I0425 21:49:11.106501   92389 kubeadm.go:576] duration metric: took 5.34790228s to wait for: map[apiserver:true system_pods:true]
I0425 21:49:11.106522   92389 node_conditions.go:102] verifying NodePressure condition ...
I0425 21:49:11.112403   92389 node_conditions.go:122] node storage ephemeral capacity is 238737052Ki
I0425 21:49:11.112423   92389 node_conditions.go:123] node cpu capacity is 4
I0425 21:49:11.112434   92389 node_conditions.go:105] duration metric: took 5.907105ms to run NodePressure ...
I0425 21:49:11.112448   92389 start.go:240] waiting for startup goroutines ...
I0425 21:49:11.112461   92389 start.go:245] waiting for cluster config update ...
I0425 21:49:11.112476   92389 start.go:254] writing updated cluster config ...
I0425 21:49:11.112862   92389 ssh_runner.go:195] Run: rm -f paused
I0425 21:49:11.302631   92389 start.go:600] kubectl: 1.30.0, cluster: 1.30.0 (minor skew: 0)
I0425 21:49:11.309374   92389 out.go:177] üèÑ  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Apr 25 16:19:11 minikube cri-dockerd[1076]: time="2024-04-25T16:19:11Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/98240a41280b06f5023bb06e7bc34394ef68522d2e3d02201bfaa3ec6002740c/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Apr 25 16:19:11 minikube cri-dockerd[1076]: time="2024-04-25T16:19:11Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/f1b8ea34da20fe2aa3809a3693de25f67c4f8088b48b16728898f32910ce3ca5/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Apr 25 16:19:11 minikube cri-dockerd[1076]: time="2024-04-25T16:19:11Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/40ea89068574f4d155e00e7cf2bc9e5f42b53ff9cb47b0ccc0074360eb021fe0/resolv.conf as [nameserver 192.168.225.1]"
Apr 25 16:19:33 minikube dockerd[844]: time="2024-04-25T16:19:33.959824752Z" level=info msg="ignoring event" container=abd62ee194c29494dff3c7223c1c03d5b4cf73a5c7e51dbadf3df33650cbc5cd module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 25 16:19:42 minikube dockerd[844]: time="2024-04-25T16:19:42.550004446Z" level=info msg="ignoring event" container=2fea2f4280339c089d3b50d635f28bf3ac49f7cceeed6433ef49b749770146d9 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 25 16:19:54 minikube dockerd[844]: time="2024-04-25T16:19:54.709426748Z" level=info msg="ignoring event" container=8f9318bf95386926c165ef7fbd2d92e4da4a4d82e11a558d495b3d4b00bd78a9 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 25 16:20:25 minikube dockerd[844]: time="2024-04-25T16:20:25.756316171Z" level=info msg="ignoring event" container=cff7b10d9d61591c156b2bceeb3cedcdeb6037f48e569c2d77aac9630a232c71 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 25 16:21:11 minikube dockerd[844]: time="2024-04-25T16:21:11.845236306Z" level=info msg="ignoring event" container=a1f16fa86e549ab89534cf3d59eaf96ffe43741cfbd460c485796d3d4aceace8 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 25 16:22:51 minikube dockerd[844]: time="2024-04-25T16:22:51.721891923Z" level=info msg="ignoring event" container=900970b51a8634162a366b2a1ebee52a75937c1b01da53f50c02e0bc5b1fc0f1 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 25 16:25:44 minikube dockerd[844]: time="2024-04-25T16:25:44.775404790Z" level=info msg="ignoring event" container=a0ceba29cc317f1baa2ceb6d2562e833a3c346cedce9b93c12b0ac597aa853a5 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 25 16:25:47 minikube cri-dockerd[1076]: time="2024-04-25T16:25:47Z" level=error msg="error getting RW layer size for container ID '900970b51a8634162a366b2a1ebee52a75937c1b01da53f50c02e0bc5b1fc0f1': Error response from daemon: No such container: 900970b51a8634162a366b2a1ebee52a75937c1b01da53f50c02e0bc5b1fc0f1"
Apr 25 16:25:47 minikube cri-dockerd[1076]: time="2024-04-25T16:25:47Z" level=error msg="Set backoffDuration to : 1m0s for container ID '900970b51a8634162a366b2a1ebee52a75937c1b01da53f50c02e0bc5b1fc0f1'"
Apr 25 16:28:25 minikube cri-dockerd[1076]: time="2024-04-25T16:28:25Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/06241de7ec21f3b7b49debed7f6090b21c8272dde7db76f285daf13ef62c5306/resolv.conf as [nameserver 10.96.0.10 search ingress-nginx.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Apr 25 16:28:25 minikube cri-dockerd[1076]: time="2024-04-25T16:28:25Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/7fa9b67807af2297c652824c6c723f28a47b6ce343e7f575939cfe58e4baaf68/resolv.conf as [nameserver 10.96.0.10 search ingress-nginx.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Apr 25 16:28:26 minikube dockerd[844]: time="2024-04-25T16:28:26.783686440Z" level=warning msg="reference for unknown type: " digest="sha256:44d1d0e9f19c63f58b380c5fddaca7cf22c7cee564adeff365225a5df5ef3334" remote="registry.k8s.io/ingress-nginx/kube-webhook-certgen@sha256:44d1d0e9f19c63f58b380c5fddaca7cf22c7cee564adeff365225a5df5ef3334" spanID=fbc5c4154b77ec93 traceID=62eec93673c95ea6adfca5c0c87c9f82
Apr 25 16:28:39 minikube cri-dockerd[1076]: time="2024-04-25T16:28:39Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.0@sha256:44d1d0e9f19c63f58b380c5fddaca7cf22c7cee564adeff365225a5df5ef3334: 74a85ff6c36f: Downloading [=>                                                 ]    686kB/22.6MB"
Apr 25 16:28:49 minikube cri-dockerd[1076]: time="2024-04-25T16:28:49Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.0@sha256:44d1d0e9f19c63f58b380c5fddaca7cf22c7cee564adeff365225a5df5ef3334: 74a85ff6c36f: Downloading [==>                                                ]  1.139MB/22.6MB"
Apr 25 16:28:59 minikube cri-dockerd[1076]: time="2024-04-25T16:28:59Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.0@sha256:44d1d0e9f19c63f58b380c5fddaca7cf22c7cee564adeff365225a5df5ef3334: 74a85ff6c36f: Downloading [====>                                              ]  2.044MB/22.6MB"
Apr 25 16:29:09 minikube cri-dockerd[1076]: time="2024-04-25T16:29:09Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.0@sha256:44d1d0e9f19c63f58b380c5fddaca7cf22c7cee564adeff365225a5df5ef3334: 74a85ff6c36f: Downloading [=======>                                           ]  3.402MB/22.6MB"
Apr 25 16:29:19 minikube cri-dockerd[1076]: time="2024-04-25T16:29:19Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.0@sha256:44d1d0e9f19c63f58b380c5fddaca7cf22c7cee564adeff365225a5df5ef3334: 74a85ff6c36f: Downloading [==========>                                        ]  4.759MB/22.6MB"
Apr 25 16:29:29 minikube cri-dockerd[1076]: time="2024-04-25T16:29:29Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.0@sha256:44d1d0e9f19c63f58b380c5fddaca7cf22c7cee564adeff365225a5df5ef3334: 74a85ff6c36f: Downloading [=============>                                     ]  6.117MB/22.6MB"
Apr 25 16:29:39 minikube cri-dockerd[1076]: time="2024-04-25T16:29:39Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.0@sha256:44d1d0e9f19c63f58b380c5fddaca7cf22c7cee564adeff365225a5df5ef3334: 74a85ff6c36f: Downloading [==================>                                ]   8.38MB/22.6MB"
Apr 25 16:29:49 minikube cri-dockerd[1076]: time="2024-04-25T16:29:49Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.0@sha256:44d1d0e9f19c63f58b380c5fddaca7cf22c7cee564adeff365225a5df5ef3334: 74a85ff6c36f: Downloading [=====================>                             ]  9.738MB/22.6MB"
Apr 25 16:29:59 minikube cri-dockerd[1076]: time="2024-04-25T16:29:59Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.0@sha256:44d1d0e9f19c63f58b380c5fddaca7cf22c7cee564adeff365225a5df5ef3334: 74a85ff6c36f: Downloading [=======================>                           ]  10.64MB/22.6MB"
Apr 25 16:30:09 minikube cri-dockerd[1076]: time="2024-04-25T16:30:09Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.0@sha256:44d1d0e9f19c63f58b380c5fddaca7cf22c7cee564adeff365225a5df5ef3334: 74a85ff6c36f: Downloading [===========================>                       ]  12.23MB/22.6MB"
Apr 25 16:30:19 minikube cri-dockerd[1076]: time="2024-04-25T16:30:19Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.0@sha256:44d1d0e9f19c63f58b380c5fddaca7cf22c7cee564adeff365225a5df5ef3334: 74a85ff6c36f: Downloading [=============================>                     ]  13.13MB/22.6MB"
Apr 25 16:30:29 minikube cri-dockerd[1076]: time="2024-04-25T16:30:29Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.0@sha256:44d1d0e9f19c63f58b380c5fddaca7cf22c7cee564adeff365225a5df5ef3334: 74a85ff6c36f: Downloading [===============================>                   ]  14.04MB/22.6MB"
Apr 25 16:30:39 minikube cri-dockerd[1076]: time="2024-04-25T16:30:39Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.0@sha256:44d1d0e9f19c63f58b380c5fddaca7cf22c7cee564adeff365225a5df5ef3334: 74a85ff6c36f: Downloading [===============================>                   ]  14.26MB/22.6MB"
Apr 25 16:30:49 minikube cri-dockerd[1076]: time="2024-04-25T16:30:49Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.0@sha256:44d1d0e9f19c63f58b380c5fddaca7cf22c7cee564adeff365225a5df5ef3334: 74a85ff6c36f: Downloading [=================================>                 ]  15.17MB/22.6MB"
Apr 25 16:30:59 minikube cri-dockerd[1076]: time="2024-04-25T16:30:59Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.0@sha256:44d1d0e9f19c63f58b380c5fddaca7cf22c7cee564adeff365225a5df5ef3334: 74a85ff6c36f: Downloading [==================================>                ]  15.62MB/22.6MB"
Apr 25 16:31:00 minikube dockerd[844]: time="2024-04-25T16:31:00.735228879Z" level=info msg="ignoring event" container=40e8921bad24b1e571e372d68b76229506dbd3ed81e888ade3571f7455b67bd3 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 25 16:31:09 minikube cri-dockerd[1076]: time="2024-04-25T16:31:09Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.0@sha256:44d1d0e9f19c63f58b380c5fddaca7cf22c7cee564adeff365225a5df5ef3334: 74a85ff6c36f: Downloading [=====================================>             ]  16.75MB/22.6MB"
Apr 25 16:31:19 minikube cri-dockerd[1076]: time="2024-04-25T16:31:19Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.0@sha256:44d1d0e9f19c63f58b380c5fddaca7cf22c7cee564adeff365225a5df5ef3334: 74a85ff6c36f: Downloading [=======================================>           ]  17.89MB/22.6MB"
Apr 25 16:31:29 minikube cri-dockerd[1076]: time="2024-04-25T16:31:29Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.0@sha256:44d1d0e9f19c63f58b380c5fddaca7cf22c7cee564adeff365225a5df5ef3334: 74a85ff6c36f: Downloading [=========================================>         ]  18.79MB/22.6MB"
Apr 25 16:31:39 minikube cri-dockerd[1076]: time="2024-04-25T16:31:39Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.0@sha256:44d1d0e9f19c63f58b380c5fddaca7cf22c7cee564adeff365225a5df5ef3334: 74a85ff6c36f: Downloading [==========================================>        ]  19.24MB/22.6MB"
Apr 25 16:31:49 minikube cri-dockerd[1076]: time="2024-04-25T16:31:49Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.0@sha256:44d1d0e9f19c63f58b380c5fddaca7cf22c7cee564adeff365225a5df5ef3334: 74a85ff6c36f: Downloading [============================================>      ]  20.15MB/22.6MB"
Apr 25 16:31:59 minikube cri-dockerd[1076]: time="2024-04-25T16:31:59Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.0@sha256:44d1d0e9f19c63f58b380c5fddaca7cf22c7cee564adeff365225a5df5ef3334: 74a85ff6c36f: Downloading [==============================================>    ]  21.05MB/22.6MB"
Apr 25 16:32:09 minikube cri-dockerd[1076]: time="2024-04-25T16:32:09Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.0@sha256:44d1d0e9f19c63f58b380c5fddaca7cf22c7cee564adeff365225a5df5ef3334: 74a85ff6c36f: Downloading [=================================================> ]  22.18MB/22.6MB"
Apr 25 16:32:13 minikube cri-dockerd[1076]: time="2024-04-25T16:32:13Z" level=info msg="Stop pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.0@sha256:44d1d0e9f19c63f58b380c5fddaca7cf22c7cee564adeff365225a5df5ef3334: Status: Downloaded newer image for registry.k8s.io/ingress-nginx/kube-webhook-certgen@sha256:44d1d0e9f19c63f58b380c5fddaca7cf22c7cee564adeff365225a5df5ef3334"
Apr 25 16:32:13 minikube dockerd[844]: time="2024-04-25T16:32:13.603154652Z" level=info msg="ignoring event" container=ceba6ebba18cc3024adca098ea455502e64344f579eb70a65fe75259778ae2ef module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 25 16:32:14 minikube cri-dockerd[1076]: time="2024-04-25T16:32:14Z" level=info msg="Stop pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.0@sha256:44d1d0e9f19c63f58b380c5fddaca7cf22c7cee564adeff365225a5df5ef3334: Status: Image is up to date for registry.k8s.io/ingress-nginx/kube-webhook-certgen@sha256:44d1d0e9f19c63f58b380c5fddaca7cf22c7cee564adeff365225a5df5ef3334"
Apr 25 16:32:14 minikube dockerd[844]: time="2024-04-25T16:32:14.368921408Z" level=info msg="ignoring event" container=bbc98d812a086be0485f9c6e79d56534171ce460720b0e8f5768cfe3d109adc6 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 25 16:32:14 minikube dockerd[844]: time="2024-04-25T16:32:14.960517530Z" level=info msg="ignoring event" container=06241de7ec21f3b7b49debed7f6090b21c8272dde7db76f285daf13ef62c5306 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 25 16:32:16 minikube dockerd[844]: time="2024-04-25T16:32:16.006877891Z" level=info msg="ignoring event" container=7fa9b67807af2297c652824c6c723f28a47b6ce343e7f575939cfe58e4baaf68 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 25 16:32:34 minikube cri-dockerd[1076]: time="2024-04-25T16:32:34Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/9371b4c4f0a848eb7fdeed646260759e7a49b155ccee1cae7c3312324c43de9b/resolv.conf as [nameserver 10.96.0.10 search ingress-nginx.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Apr 25 16:32:36 minikube dockerd[844]: time="2024-04-25T16:32:36.149687835Z" level=warning msg="reference for unknown type: " digest="sha256:42b3f0e5d0846876b1791cd3afeb5f1cbbe4259d6f35651dcc1b5c980925379c" remote="registry.k8s.io/ingress-nginx/controller@sha256:42b3f0e5d0846876b1791cd3afeb5f1cbbe4259d6f35651dcc1b5c980925379c" spanID=4c7d73537ecd205c traceID=79dfbf7eb9d570bfcb37c2524de02e1f
Apr 25 16:32:48 minikube cri-dockerd[1076]: time="2024-04-25T16:32:48Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.10.0@sha256:42b3f0e5d0846876b1791cd3afeb5f1cbbe4259d6f35651dcc1b5c980925379c: 4abcf2066143: Downloading [============>                                      ]    835kB/3.409MB"
Apr 25 16:32:58 minikube cri-dockerd[1076]: time="2024-04-25T16:32:58Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.10.0@sha256:42b3f0e5d0846876b1791cd3afeb5f1cbbe4259d6f35651dcc1b5c980925379c: 4abcf2066143: Downloading [=========================>                         ]  1.705MB/3.409MB"
Apr 25 16:33:08 minikube cri-dockerd[1076]: time="2024-04-25T16:33:08Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.10.0@sha256:42b3f0e5d0846876b1791cd3afeb5f1cbbe4259d6f35651dcc1b5c980925379c: 4abcf2066143: Downloading [=================================>                 ]  2.262MB/3.409MB"
Apr 25 16:33:18 minikube cri-dockerd[1076]: time="2024-04-25T16:33:18Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.10.0@sha256:42b3f0e5d0846876b1791cd3afeb5f1cbbe4259d6f35651dcc1b5c980925379c: 4abcf2066143: Downloading [=======================================>           ]   2.68MB/3.409MB"
Apr 25 16:33:28 minikube cri-dockerd[1076]: time="2024-04-25T16:33:28Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.10.0@sha256:42b3f0e5d0846876b1791cd3afeb5f1cbbe4259d6f35651dcc1b5c980925379c: cb71730f6575: Downloading [================================================>  ]   4.75MB/4.922MB"
Apr 25 16:33:38 minikube cri-dockerd[1076]: time="2024-04-25T16:33:38Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.10.0@sha256:42b3f0e5d0846876b1791cd3afeb5f1cbbe4259d6f35651dcc1b5c980925379c: 4f4fb700ef54: Download complete "
Apr 25 16:33:48 minikube cri-dockerd[1076]: time="2024-04-25T16:33:48Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.10.0@sha256:42b3f0e5d0846876b1791cd3afeb5f1cbbe4259d6f35651dcc1b5c980925379c: 3bbe4fa9abc9: Downloading [=============>                                     ]  867.7kB/3.108MB"
Apr 25 16:33:58 minikube cri-dockerd[1076]: time="2024-04-25T16:33:58Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.10.0@sha256:42b3f0e5d0846876b1791cd3afeb5f1cbbe4259d6f35651dcc1b5c980925379c: 4d17307cc386: Downloading [============>                                      ]  8.336MB/33.94MB"
Apr 25 16:34:08 minikube cri-dockerd[1076]: time="2024-04-25T16:34:08Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.10.0@sha256:42b3f0e5d0846876b1791cd3afeb5f1cbbe4259d6f35651dcc1b5c980925379c: 3bbe4fa9abc9: Downloading [=================================================> ]  3.096MB/3.108MB"
Apr 25 16:34:18 minikube cri-dockerd[1076]: time="2024-04-25T16:34:18Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.10.0@sha256:42b3f0e5d0846876b1791cd3afeb5f1cbbe4259d6f35651dcc1b5c980925379c: d755c7edfc06: Downloading [==>                                                ]  555.4kB/13.37MB"
Apr 25 16:34:28 minikube cri-dockerd[1076]: time="2024-04-25T16:34:28Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.10.0@sha256:42b3f0e5d0846876b1791cd3afeb5f1cbbe4259d6f35651dcc1b5c980925379c: 3fddda0d1cd8: Downloading [=============>                                     ]  5.168MB/18.84MB"
Apr 25 16:34:38 minikube cri-dockerd[1076]: time="2024-04-25T16:34:38Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.10.0@sha256:42b3f0e5d0846876b1791cd3afeb5f1cbbe4259d6f35651dcc1b5c980925379c: 4d17307cc386: Downloading [=================>                                 ]  12.17MB/33.94MB"
Apr 25 16:34:48 minikube cri-dockerd[1076]: time="2024-04-25T16:34:48Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.10.0@sha256:42b3f0e5d0846876b1791cd3afeb5f1cbbe4259d6f35651dcc1b5c980925379c: d755c7edfc06: Downloading [================>                                  ]  4.316MB/13.37MB"
Apr 25 16:34:58 minikube cri-dockerd[1076]: time="2024-04-25T16:34:58Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.10.0@sha256:42b3f0e5d0846876b1791cd3afeb5f1cbbe4259d6f35651dcc1b5c980925379c: d755c7edfc06: Downloading [======================>                            ]  6.126MB/13.37MB"


==> container status <==
CONTAINER           IMAGE                                                                                                                        CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
bbc98d812a086       registry.k8s.io/ingress-nginx/kube-webhook-certgen@sha256:44d1d0e9f19c63f58b380c5fddaca7cf22c7cee564adeff365225a5df5ef3334   2 minutes ago       Exited              patch                     0                   7fa9b67807af2       ingress-nginx-admission-patch-zsncs
ceba6ebba18cc       registry.k8s.io/ingress-nginx/kube-webhook-certgen@sha256:44d1d0e9f19c63f58b380c5fddaca7cf22c7cee564adeff365225a5df5ef3334   2 minutes ago       Exited              create                    0                   06241de7ec21f       ingress-nginx-admission-create-8pfgv
40e8921bad24b       3fad35d6afb00                                                                                                                4 minutes ago       Exited              blog-web                  12                  f1b8ea34da20f       blog-web-66455df769-f6cs8
e75deb31ebfe2       6e38f40d628db                                                                                                                15 minutes ago      Running             storage-provisioner       2                   444bf4517af29       storage-provisioner
39973a066a55f       cbb01a7bd410d                                                                                                                15 minutes ago      Running             coredns                   1                   40ea89068574f       coredns-7db6d8ff4d-c86rh
ab1a31c5164b3       a0bf559e280cf                                                                                                                15 minutes ago      Running             kube-proxy                1                   8267597a47635       kube-proxy-smrmv
9a039b904b4c2       ebf01b748a56f                                                                                                                15 minutes ago      Running             db                        1                   98240a41280b0       blog-postgres-0
2fea2f4280339       6e38f40d628db                                                                                                                15 minutes ago      Exited              storage-provisioner       1                   444bf4517af29       storage-provisioner
14080f7fbc406       3861cfcd7c04c                                                                                                                15 minutes ago      Running             etcd                      1                   ba0e3a8b4f75c       etcd-minikube
5a36232204114       c42f13656d0b2                                                                                                                15 minutes ago      Running             kube-apiserver            1                   2301f86690fb7       kube-apiserver-minikube
a6dc1dacc158d       259c8277fcbbc                                                                                                                15 minutes ago      Running             kube-scheduler            1                   78fa3a630ae23       kube-scheduler-minikube
2ac223c654ad2       c7aad43836fa5                                                                                                                15 minutes ago      Running             kube-controller-manager   1                   d61c1ad7b16ec       kube-controller-manager-minikube
1cd348ce388d0       ebf01b748a56f                                                                                                                30 hours ago        Exited              db                        0                   12debce58ec8f       blog-postgres-0
4ccaf1c5199f1       cbb01a7bd410d                                                                                                                30 hours ago        Exited              coredns                   0                   64da6625d347f       coredns-7db6d8ff4d-c86rh
08d1ea4df6267       a0bf559e280cf                                                                                                                30 hours ago        Exited              kube-proxy                0                   3b82fcf8181f8       kube-proxy-smrmv
fb77d699ecd74       c7aad43836fa5                                                                                                                30 hours ago        Exited              kube-controller-manager   0                   e99a2d82d7f0f       kube-controller-manager-minikube
71aa92278060d       c42f13656d0b2                                                                                                                30 hours ago        Exited              kube-apiserver            0                   a564e06b998e8       kube-apiserver-minikube
a647d31a6975e       259c8277fcbbc                                                                                                                30 hours ago        Exited              kube-scheduler            0                   ecdd1a12992c6       kube-scheduler-minikube
d1d8d5895bf9c       3861cfcd7c04c                                                                                                                30 hours ago        Exited              etcd                      0                   898410b8830cc       etcd-minikube


==> coredns [39973a066a55] <==
[INFO] 10.244.0.13:57238 - 41677 "AAAA IN blog-postgres.cluster.local. udp 45 false 512" NXDOMAIN qr,aa,rd 138 0.000105619s
[INFO] 10.244.0.13:57238 - 13811 "A IN blog-postgres.cluster.local. udp 45 false 512" NXDOMAIN qr,aa,rd 138 0.000149733s
[INFO] 10.244.0.13:51988 - 22367 "AAAA IN blog-postgres. udp 31 false 512" NXDOMAIN qr,rd,ra 31 0.017060454s
[INFO] 10.244.0.13:51988 - 14429 "A IN blog-postgres. udp 31 false 512" NXDOMAIN qr,rd,ra 31 0.017295608s
[INFO] 10.244.0.13:44029 - 5407 "A IN blog-postgres.default.svc.cluster.local. udp 57 false 512" NXDOMAIN qr,aa,rd 150 0.000179312s
[INFO] 10.244.0.13:44029 - 65303 "AAAA IN blog-postgres.default.svc.cluster.local. udp 57 false 512" NXDOMAIN qr,aa,rd 150 0.00034296s
[INFO] 10.244.0.13:49960 - 49391 "AAAA IN blog-postgres.svc.cluster.local. udp 49 false 512" NXDOMAIN qr,aa,rd 142 0.000188853s
[INFO] 10.244.0.13:49960 - 60052 "A IN blog-postgres.svc.cluster.local. udp 49 false 512" NXDOMAIN qr,aa,rd 142 0.000295072s
[INFO] 10.244.0.13:56796 - 26486 "AAAA IN blog-postgres.cluster.local. udp 45 false 512" NXDOMAIN qr,aa,rd 138 0.000147149s
[INFO] 10.244.0.13:56796 - 49481 "A IN blog-postgres.cluster.local. udp 45 false 512" NXDOMAIN qr,aa,rd 138 0.000269395s
[INFO] 10.244.0.13:40092 - 39018 "AAAA IN blog-postgres. udp 31 false 512" NXDOMAIN qr,rd,ra 31 0.00884706s
[INFO] 10.244.0.13:40092 - 878 "A IN blog-postgres. udp 31 false 512" NXDOMAIN qr,rd,ra 31 0.008895176s
[INFO] 10.244.0.13:42682 - 11799 "A IN blog-postgres.default.svc.cluster.local. udp 57 false 512" NXDOMAIN qr,aa,rd 150 0.000345746s
[INFO] 10.244.0.13:42682 - 282 "AAAA IN blog-postgres.default.svc.cluster.local. udp 57 false 512" NXDOMAIN qr,aa,rd 150 0.000713304s
[INFO] 10.244.0.13:45960 - 31226 "AAAA IN blog-postgres.svc.cluster.local. udp 49 false 512" NXDOMAIN qr,aa,rd 142 0.000276494s
[INFO] 10.244.0.13:45960 - 8679 "A IN blog-postgres.svc.cluster.local. udp 49 false 512" NXDOMAIN qr,aa,rd 142 0.000281244s
[INFO] 10.244.0.13:41551 - 22019 "AAAA IN blog-postgres.cluster.local. udp 45 false 512" NXDOMAIN qr,aa,rd 138 0.000265538s
[INFO] 10.244.0.13:41551 - 15157 "A IN blog-postgres.cluster.local. udp 45 false 512" NXDOMAIN qr,aa,rd 138 0.000433334s
[INFO] 10.244.0.13:50868 - 13514 "AAAA IN blog-postgres. udp 31 false 512" NXDOMAIN qr,rd,ra 31 0.012153281s
[INFO] 10.244.0.13:50868 - 44480 "A IN blog-postgres. udp 31 false 512" NXDOMAIN qr,rd,ra 31 0.062752907s
[INFO] 10.244.0.13:45746 - 10513 "AAAA IN blog-postgres.default.svc.cluster.local. udp 57 false 512" NXDOMAIN qr,aa,rd 150 0.00041955s
[INFO] 10.244.0.13:45746 - 52193 "A IN blog-postgres.default.svc.cluster.local. udp 57 false 512" NXDOMAIN qr,aa,rd 150 0.000767329s
[INFO] 10.244.0.13:37902 - 3903 "AAAA IN blog-postgres.svc.cluster.local. udp 49 false 512" NXDOMAIN qr,aa,rd 142 0.000336206s
[INFO] 10.244.0.13:37902 - 60205 "A IN blog-postgres.svc.cluster.local. udp 49 false 512" NXDOMAIN qr,aa,rd 142 0.000243751s
[INFO] 10.244.0.13:55280 - 46486 "A IN blog-postgres.cluster.local. udp 45 false 512" NXDOMAIN qr,aa,rd 138 0.000844493s
[INFO] 10.244.0.13:55280 - 51101 "AAAA IN blog-postgres.cluster.local. udp 45 false 512" NXDOMAIN qr,aa,rd 138 0.000382367s
[INFO] 10.244.0.13:46737 - 35520 "A IN blog-postgres. udp 31 false 512" NXDOMAIN qr,rd,ra 31 0.007579563s
[INFO] 10.244.0.13:46737 - 35002 "AAAA IN blog-postgres. udp 31 false 512" NXDOMAIN qr,rd,ra 31 0.008199324s
[INFO] 10.244.0.13:37802 - 52183 "AAAA IN blog-postgres.default.svc.cluster.local. udp 57 false 512" NXDOMAIN qr,aa,rd 150 0.000178881s
[INFO] 10.244.0.13:37802 - 43220 "A IN blog-postgres.default.svc.cluster.local. udp 57 false 512" NXDOMAIN qr,aa,rd 150 0.000255572s
[INFO] 10.244.0.13:37800 - 27479 "AAAA IN blog-postgres.svc.cluster.local. udp 49 false 512" NXDOMAIN qr,aa,rd 142 0.000123298s
[INFO] 10.244.0.13:37800 - 36954 "A IN blog-postgres.svc.cluster.local. udp 49 false 512" NXDOMAIN qr,aa,rd 142 0.000168349s
[INFO] 10.244.0.13:47800 - 24334 "AAAA IN blog-postgres.cluster.local. udp 45 false 512" NXDOMAIN qr,aa,rd 138 0.00014702s
[INFO] 10.244.0.13:47800 - 28683 "A IN blog-postgres.cluster.local. udp 45 false 512" NXDOMAIN qr,aa,rd 138 0.000261804s
[INFO] 10.244.0.13:50715 - 59896 "A IN blog-postgres. udp 31 false 512" NXDOMAIN qr,rd,ra 106 0.090369684s
[INFO] 10.244.0.13:50715 - 20474 "AAAA IN blog-postgres. udp 31 false 512" NXDOMAIN qr,rd,ra 106 0.090568746s
[INFO] 10.244.0.13:33663 - 13287 "AAAA IN blog-postgres.default.svc.cluster.local. udp 57 false 512" NXDOMAIN qr,aa,rd 150 0.000292101s
[INFO] 10.244.0.13:33663 - 44560 "A IN blog-postgres.default.svc.cluster.local. udp 57 false 512" NXDOMAIN qr,aa,rd 150 0.000486492s
[INFO] 10.244.0.13:47115 - 4825 "AAAA IN blog-postgres.svc.cluster.local. udp 49 false 512" NXDOMAIN qr,aa,rd 142 0.000411857s
[INFO] 10.244.0.13:47115 - 56268 "A IN blog-postgres.svc.cluster.local. udp 49 false 512" NXDOMAIN qr,aa,rd 142 0.000481443s
[INFO] 10.244.0.13:35305 - 56462 "A IN blog-postgres.cluster.local. udp 45 false 512" NXDOMAIN qr,aa,rd 138 0.00028265s
[INFO] 10.244.0.13:35305 - 9465 "AAAA IN blog-postgres.cluster.local. udp 45 false 512" NXDOMAIN qr,aa,rd 138 0.000224878s
[INFO] 10.244.0.13:43429 - 9069 "AAAA IN blog-postgres. udp 31 false 512" NXDOMAIN qr,aa,rd,ra 106 0.000203271s
[INFO] 10.244.0.13:43429 - 2674 "A IN blog-postgres. udp 31 false 512" NXDOMAIN qr,aa,rd,ra 106 0.000389623s
[INFO] 10.244.0.13:57905 - 45610 "A IN blog-postgres.default.svc.cluster.local. udp 57 false 512" NXDOMAIN qr,aa,rd 150 0.000294174s
[INFO] 10.244.0.13:57905 - 25894 "AAAA IN blog-postgres.default.svc.cluster.local. udp 57 false 512" NXDOMAIN qr,aa,rd 150 0.000532227s
[INFO] 10.244.0.13:39311 - 42711 "AAAA IN blog-postgres.svc.cluster.local. udp 49 false 512" NXDOMAIN qr,aa,rd 142 0.000242522s
[INFO] 10.244.0.13:39311 - 44067 "A IN blog-postgres.svc.cluster.local. udp 49 false 512" NXDOMAIN qr,aa,rd 142 0.000431497s
[INFO] 10.244.0.13:45242 - 32516 "AAAA IN blog-postgres.cluster.local. udp 45 false 512" NXDOMAIN qr,aa,rd 138 0.000240553s
[INFO] 10.244.0.13:45242 - 10511 "A IN blog-postgres.cluster.local. udp 45 false 512" NXDOMAIN qr,aa,rd 138 0.000498429s
[INFO] 10.244.0.13:47269 - 51251 "A IN blog-postgres. udp 31 false 512" NXDOMAIN qr,aa,rd,ra 106 0.000223419s
[INFO] 10.244.0.13:47269 - 31787 "AAAA IN blog-postgres. udp 31 false 512" NXDOMAIN qr,aa,rd,ra 106 0.001585649s
[INFO] 10.244.0.13:36443 - 62873 "A IN blog-postgres.default.svc.cluster.local. udp 57 false 512" NXDOMAIN qr,aa,rd 150 0.000306097s
[INFO] 10.244.0.13:36443 - 38038 "AAAA IN blog-postgres.default.svc.cluster.local. udp 57 false 512" NXDOMAIN qr,aa,rd 150 0.000543644s
[INFO] 10.244.0.13:59133 - 21402 "AAAA IN blog-postgres.svc.cluster.local. udp 49 false 512" NXDOMAIN qr,aa,rd 142 0.000223525s
[INFO] 10.244.0.13:59133 - 39552 "A IN blog-postgres.svc.cluster.local. udp 49 false 512" NXDOMAIN qr,aa,rd 142 0.000403125s
[INFO] 10.244.0.13:60816 - 12575 "AAAA IN blog-postgres.cluster.local. udp 45 false 512" NXDOMAIN qr,aa,rd 138 0.000222325s
[INFO] 10.244.0.13:60816 - 12577 "A IN blog-postgres.cluster.local. udp 45 false 512" NXDOMAIN qr,aa,rd 138 0.000341828s
[INFO] 10.244.0.13:34486 - 22580 "AAAA IN blog-postgres. udp 31 false 512" NXDOMAIN qr,aa,rd,ra 106 0.00022053s
[INFO] 10.244.0.13:34486 - 30771 "A IN blog-postgres. udp 31 false 512" NXDOMAIN qr,aa,rd,ra 106 0.000389296s


==> coredns [4ccaf1c5199f] <==
[INFO] 10.244.0.11:58804 - 22092 "AAAA IN blog-postgres. udp 31 false 512" NXDOMAIN qr,rd,ra 31 0.003656009s
[INFO] 10.244.0.11:58804 - 39241 "A IN blog-postgres. udp 31 false 512" NXDOMAIN qr,rd,ra 31 0.003757253s
[INFO] 10.244.0.11:50484 - 62286 "AAAA IN blog-postgres.default.svc.cluster.local. udp 57 false 512" NXDOMAIN qr,aa,rd 150 0.000086252s
[INFO] 10.244.0.11:50484 - 34633 "A IN blog-postgres.default.svc.cluster.local. udp 57 false 512" NXDOMAIN qr,aa,rd 150 0.000257194s
[INFO] 10.244.0.11:51301 - 27680 "A IN blog-postgres.svc.cluster.local. udp 49 false 512" NXDOMAIN qr,aa,rd 142 0.000072102s
[INFO] 10.244.0.11:51301 - 26659 "AAAA IN blog-postgres.svc.cluster.local. udp 49 false 512" NXDOMAIN qr,aa,rd 142 0.000073499s
[INFO] 10.244.0.11:37219 - 54566 "AAAA IN blog-postgres.cluster.local. udp 45 false 512" NXDOMAIN qr,aa,rd 138 0.000065587s
[INFO] 10.244.0.11:37219 - 13860 "A IN blog-postgres.cluster.local. udp 45 false 512" NXDOMAIN qr,aa,rd 138 0.000113445s
[INFO] 10.244.0.11:58970 - 12994 "AAAA IN blog-postgres. udp 31 false 512" NXDOMAIN qr,rd,ra 31 0.003531125s
[INFO] 10.244.0.11:58970 - 48576 "A IN blog-postgres. udp 31 false 512" NXDOMAIN qr,rd,ra 31 0.00358799s
[INFO] 10.244.0.11:45349 - 35427 "AAAA IN blog-postgres.default.svc.cluster.local. udp 57 false 512" NXDOMAIN qr,aa,rd 150 0.000105909s
[INFO] 10.244.0.11:45349 - 18279 "A IN blog-postgres.default.svc.cluster.local. udp 57 false 512" NXDOMAIN qr,aa,rd 150 0.00016934s
[INFO] 10.244.0.11:41142 - 19512 "AAAA IN blog-postgres.svc.cluster.local. udp 49 false 512" NXDOMAIN qr,aa,rd 142 0.000093095s
[INFO] 10.244.0.11:41142 - 24379 "A IN blog-postgres.svc.cluster.local. udp 49 false 512" NXDOMAIN qr,aa,rd 142 0.000168992s
[INFO] 10.244.0.11:36149 - 38309 "A IN blog-postgres.cluster.local. udp 45 false 512" NXDOMAIN qr,aa,rd 138 0.000066406s
[INFO] 10.244.0.11:36149 - 29880 "AAAA IN blog-postgres.cluster.local. udp 45 false 512" NXDOMAIN qr,aa,rd 138 0.00011274s
[INFO] 10.244.0.11:39884 - 4857 "AAAA IN blog-postgres. udp 31 false 512" NXDOMAIN qr,rd,ra 31 0.141459561s
[INFO] 10.244.0.11:39884 - 2555 "A IN blog-postgres. udp 31 false 512" NXDOMAIN qr,rd,ra 31 0.141830593s
[INFO] 10.244.0.11:54355 - 27993 "AAAA IN blog-postgres.default.svc.cluster.local. udp 57 false 512" NXDOMAIN qr,aa,rd 150 0.000206052s
[INFO] 10.244.0.11:54355 - 62034 "A IN blog-postgres.default.svc.cluster.local. udp 57 false 512" NXDOMAIN qr,aa,rd 150 0.000613414s
[INFO] 10.244.0.11:44394 - 27141 "AAAA IN blog-postgres.svc.cluster.local. udp 49 false 512" NXDOMAIN qr,aa,rd 142 0.000275816s
[INFO] 10.244.0.11:44394 - 43036 "A IN blog-postgres.svc.cluster.local. udp 49 false 512" NXDOMAIN qr,aa,rd 142 0.000471933s
[INFO] 10.244.0.11:39456 - 63009 "A IN blog-postgres.cluster.local. udp 45 false 512" NXDOMAIN qr,aa,rd 138 0.000217568s
[INFO] 10.244.0.11:39456 - 22872 "AAAA IN blog-postgres.cluster.local. udp 45 false 512" NXDOMAIN qr,aa,rd 138 0.000526239s
[INFO] 10.244.0.11:56850 - 58453 "A IN blog-postgres. udp 31 false 512" NXDOMAIN qr,rd,ra 31 0.00325088s
[INFO] 10.244.0.11:56850 - 24146 "AAAA IN blog-postgres. udp 31 false 512" NXDOMAIN qr,rd,ra 31 0.003743305s
[INFO] 10.244.0.11:60058 - 23019 "A IN blog-postgres.default.svc.cluster.local. udp 57 false 512" NXDOMAIN qr,aa,rd 150 0.00013492s
[INFO] 10.244.0.11:60058 - 40424 "AAAA IN blog-postgres.default.svc.cluster.local. udp 57 false 512" NXDOMAIN qr,aa,rd 150 0.000326902s
[INFO] 10.244.0.11:35547 - 38338 "A IN blog-postgres.svc.cluster.local. udp 49 false 512" NXDOMAIN qr,aa,rd 142 0.000103866s
[INFO] 10.244.0.11:35547 - 64198 "AAAA IN blog-postgres.svc.cluster.local. udp 49 false 512" NXDOMAIN qr,aa,rd 142 0.000158952s
[INFO] 10.244.0.11:33701 - 18563 "A IN blog-postgres.cluster.local. udp 45 false 512" NXDOMAIN qr,aa,rd 138 0.000110206s
[INFO] 10.244.0.11:33701 - 14977 "AAAA IN blog-postgres.cluster.local. udp 45 false 512" NXDOMAIN qr,aa,rd 138 0.000171992s
[INFO] 10.244.0.11:48905 - 36674 "A IN blog-postgres. udp 31 false 512" NXDOMAIN qr,rd,ra 31 0.005160117s
[INFO] 10.244.0.11:48905 - 605 "AAAA IN blog-postgres. udp 31 false 512" NXDOMAIN qr,rd,ra 31 0.005254068s
[INFO] 10.244.0.11:41958 - 49004 "AAAA IN blog-postgres.default.svc.cluster.local. udp 57 false 512" NXDOMAIN qr,aa,rd 150 0.000075683s
[INFO] 10.244.0.11:41958 - 61551 "A IN blog-postgres.default.svc.cluster.local. udp 57 false 512" NXDOMAIN qr,aa,rd 150 0.000082287s
[INFO] 10.244.0.11:38004 - 31298 "AAAA IN blog-postgres.svc.cluster.local. udp 49 false 512" NXDOMAIN qr,aa,rd 142 0.000067856s
[INFO] 10.244.0.11:38004 - 5199 "A IN blog-postgres.svc.cluster.local. udp 49 false 512" NXDOMAIN qr,aa,rd 142 0.000221818s
[INFO] 10.244.0.11:43719 - 5964 "A IN blog-postgres.cluster.local. udp 45 false 512" NXDOMAIN qr,aa,rd 138 0.000074951s
[INFO] 10.244.0.11:43719 - 59215 "AAAA IN blog-postgres.cluster.local. udp 45 false 512" NXDOMAIN qr,aa,rd 138 0.000102638s
[INFO] 10.244.0.11:48692 - 54228 "AAAA IN blog-postgres. udp 31 false 512" NXDOMAIN qr,rd,ra 31 0.003812757s
[INFO] 10.244.0.11:48692 - 23763 "A IN blog-postgres. udp 31 false 512" NXDOMAIN qr,rd,ra 31 0.003953358s
[INFO] 10.244.0.11:41430 - 19653 "AAAA IN blog-postgres.default.svc.cluster.local. udp 57 false 512" NXDOMAIN qr,aa,rd 150 0.000283924s
[INFO] 10.244.0.11:41430 - 29132 "A IN blog-postgres.default.svc.cluster.local. udp 57 false 512" NXDOMAIN qr,aa,rd 150 0.000815851s
[INFO] 10.244.0.11:48198 - 534 "AAAA IN blog-postgres.svc.cluster.local. udp 49 false 512" NXDOMAIN qr,aa,rd 142 0.000255764s
[INFO] 10.244.0.11:48198 - 57629 "A IN blog-postgres.svc.cluster.local. udp 49 false 512" NXDOMAIN qr,aa,rd 142 0.000727782s
[INFO] 10.244.0.11:32791 - 20648 "A IN blog-postgres.cluster.local. udp 45 false 512" NXDOMAIN qr,aa,rd 138 0.000319051s
[INFO] 10.244.0.11:32791 - 27810 "AAAA IN blog-postgres.cluster.local. udp 45 false 512" NXDOMAIN qr,aa,rd 138 0.000264164s
[INFO] 10.244.0.11:45026 - 9188 "AAAA IN blog-postgres. udp 31 false 512" NXDOMAIN qr,rd,ra 31 0.005642617s
[INFO] 10.244.0.11:45026 - 18927 "A IN blog-postgres. udp 31 false 512" NXDOMAIN qr,rd,ra 31 0.005772889s
[INFO] 10.244.0.11:59398 - 59374 "AAAA IN blog-postgres.default.svc.cluster.local. udp 57 false 512" NXDOMAIN qr,aa,rd 150 0.000313448s
[INFO] 10.244.0.11:59398 - 4080 "A IN blog-postgres.default.svc.cluster.local. udp 57 false 512" NXDOMAIN qr,aa,rd 150 0.000466624s
[INFO] 10.244.0.11:35153 - 54738 "A IN blog-postgres.svc.cluster.local. udp 49 false 512" NXDOMAIN qr,aa,rd 142 0.0002499s
[INFO] 10.244.0.11:35153 - 9764 "AAAA IN blog-postgres.svc.cluster.local. udp 49 false 512" NXDOMAIN qr,aa,rd 142 0.000540473s
[INFO] 10.244.0.11:60543 - 19019 "A IN blog-postgres.cluster.local. udp 45 false 512" NXDOMAIN qr,aa,rd 138 0.000195269s
[INFO] 10.244.0.11:60543 - 35906 "AAAA IN blog-postgres.cluster.local. udp 45 false 512" NXDOMAIN qr,aa,rd 138 0.00033568s
[INFO] 10.244.0.11:49131 - 4794 "AAAA IN blog-postgres. udp 31 false 512" NXDOMAIN qr,rd,ra 31 0.004633198s
[INFO] 10.244.0.11:49131 - 27298 "A IN blog-postgres. udp 31 false 512" NXDOMAIN qr,rd,ra 31 0.00745933s
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=86fc9d54fca63f295d8737c8eacdbb7987e89c67
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2024_04_24T15_39_12_0700
                    minikube.k8s.io/version=v1.33.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Wed, 24 Apr 2024 10:09:09 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Thu, 25 Apr 2024 16:35:00 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Thu, 25 Apr 2024 16:32:45 +0000   Wed, 24 Apr 2024 10:09:08 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Thu, 25 Apr 2024 16:32:45 +0000   Wed, 24 Apr 2024 10:09:08 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Thu, 25 Apr 2024 16:32:45 +0000   Wed, 24 Apr 2024 10:09:08 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Thu, 25 Apr 2024 16:32:45 +0000   Wed, 24 Apr 2024 10:09:09 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  172.17.0.2
  Hostname:    minikube
Capacity:
  cpu:                4
  ephemeral-storage:  238737052Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             12011096Ki
  pods:               110
Allocatable:
  cpu:                4
  ephemeral-storage:  238737052Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             12011096Ki
  pods:               110
System Info:
  Machine ID:                 31f2799f22b34cb5bb8eff04fbca13f3
  System UUID:                33dd5725-6576-42a0-accd-9b1ee17166e3
  Boot ID:                    6114faf0-d25f-4f3d-be61-d0102c6d9db9
  Kernel Version:             6.5.0-28-generic
  OS Image:                   Ubuntu 22.04.4 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://26.0.1
  Kubelet Version:            v1.30.0
  Kube-Proxy Version:         v1.30.0
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (10 in total)
  Namespace                   Name                                        CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                        ------------  ----------  ---------------  -------------  ---
  default                     blog-postgres-0                             0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         29h
  default                     blog-web-66455df769-f6cs8                   200m (5%!)(MISSING)     500m (12%!)(MISSING)  256Mi (2%!)(MISSING)       512Mi (4%!)(MISSING)     29h
  ingress-nginx               ingress-nginx-controller-84df5799c-8lrhc    100m (2%!)(MISSING)     0 (0%!)(MISSING)      90Mi (0%!)(MISSING)        0 (0%!)(MISSING)         6m40s
  kube-system                 coredns-7db6d8ff4d-c86rh                    100m (2%!)(MISSING)     0 (0%!)(MISSING)      70Mi (0%!)(MISSING)        170Mi (1%!)(MISSING)     30h
  kube-system                 etcd-minikube                               100m (2%!)(MISSING)     0 (0%!)(MISSING)      100Mi (0%!)(MISSING)       0 (0%!)(MISSING)         30h
  kube-system                 kube-apiserver-minikube                     250m (6%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         30h
  kube-system                 kube-controller-manager-minikube            200m (5%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         30h
  kube-system                 kube-proxy-smrmv                            0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         30h
  kube-system                 kube-scheduler-minikube                     100m (2%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         30h
  kube-system                 storage-provisioner                         0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         30h
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests     Limits
  --------           --------     ------
  cpu                1050m (26%!)(MISSING)  500m (12%!)(MISSING)
  memory             516Mi (4%!)(MISSING)   682Mi (5%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)       0 (0%!)(MISSING)
  hugepages-1Gi      0 (0%!)(MISSING)       0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)       0 (0%!)(MISSING)
Events:
  Type    Reason                   Age                From             Message
  ----    ------                   ----               ----             -------
  Normal  Starting                 15m                kube-proxy       
  Normal  Starting                 15m                kubelet          Starting kubelet.
  Normal  NodeHasSufficientMemory  15m (x8 over 15m)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    15m (x8 over 15m)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     15m (x7 over 15m)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  NodeAllocatableEnforced  15m                kubelet          Updated Node Allocatable limit across pods
  Normal  RegisteredNode           15m                node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
[  +1.428751] rtw_8723de 0000:02:00.0: failed to get tx report from firmware
[  +9.622829] rtw_8723de 0000:02:00.0: failed to get tx report from firmware
[  +8.381554] rtw_8723de 0000:02:00.0: timed out to flush queue 1
[  +1.713974] rtw_8723de 0000:02:00.0: failed to get tx report from firmware
[  +9.912209] rtw_8723de 0000:02:00.0: failed to get tx report from firmware
[  +9.916479] rtw_8723de 0000:02:00.0: failed to get tx report from firmware
[  +7.011071] rtw_8723de 0000:02:00.0: timed out to flush queue 1
[Apr25 16:28] rtw_8723de 0000:02:00.0: failed to get tx report from firmware
[  +9.663997] rtw_8723de 0000:02:00.0: failed to get tx report from firmware
[ +10.016683] rtw_8723de 0000:02:00.0: failed to get tx report from firmware
[  +9.982535] rtw_8723de 0000:02:00.0: failed to get tx report from firmware
[  +9.951025] rtw_8723de 0000:02:00.0: failed to get tx report from firmware
[ +10.047506] rtw_8723de 0000:02:00.0: failed to get tx report from firmware
[  +9.951925] rtw_8723de 0000:02:00.0: failed to get tx report from firmware
[Apr25 16:29] rtw_8723de 0000:02:00.0: timed out to flush queue 1
[  +2.828088] rtw_8723de 0000:02:00.0: failed to get tx report from firmware
[  +9.824407] rtw_8723de 0000:02:00.0: failed to get tx report from firmware
[  +9.952605] rtw_8723de 0000:02:00.0: failed to get tx report from firmware
[ +10.080702] rtw_8723de 0000:02:00.0: failed to get tx report from firmware
[  +9.956928] rtw_8723de 0000:02:00.0: failed to get tx report from firmware
[  +9.952704] rtw_8723de 0000:02:00.0: failed to get tx report from firmware
[Apr25 16:30] rtw_8723de 0000:02:00.0: timed out to flush queue 1
[  +2.492079] rtw_8723de 0000:02:00.0: failed to get tx report from firmware
[  +2.176223] rtw_8723de 0000:02:00.0: failed to get tx report from firmware
[  +7.360686] rtw_8723de 0000:02:00.0: failed to get tx report from firmware
[  +9.064926] rtw_8723de 0000:02:00.0: timed out to flush queue 1
[  +0.147979] rtw_8723de 0000:02:00.0: timed out to flush queue 1
[  +0.995639] rtw_8723de 0000:02:00.0: failed to get tx report from firmware
[  +9.195152] rtw_8723de 0000:02:00.0: timed out to flush queue 1
[  +1.039478] rtw_8723de 0000:02:00.0: failed to get tx report from firmware
[  +7.988431] rtw_8723de 0000:02:00.0: timed out to flush queue 1
[  +1.831231] rtw_8723de 0000:02:00.0: failed to get tx report from firmware
[  +9.756330] rtw_8723de 0000:02:00.0: failed to get tx report from firmware
[Apr25 16:31] rtw_8723de 0000:02:00.0: timed out to flush queue 1
[  +3.574849] rtw_8723de 0000:02:00.0: failed to get tx report from firmware
[  +9.437479] rtw_8723de 0000:02:00.0: failed to get tx report from firmware
[  +9.949731] rtw_8723de 0000:02:00.0: failed to get tx report from firmware
[ +10.018140] rtw_8723de 0000:02:00.0: failed to get tx report from firmware
[  +7.018927] rtw_8723de 0000:02:00.0: timed out to flush queue 1
[  +3.471447] rtw_8723de 0000:02:00.0: failed to get tx report from firmware
[  +9.662725] rtw_8723de 0000:02:00.0: failed to get tx report from firmware
[Apr25 16:32] rtw_8723de 0000:02:00.0: failed to get tx report from firmware
[ +10.047135] rtw_8723de 0000:02:00.0: failed to get tx report from firmware
[  +9.983248] rtw_8723de 0000:02:00.0: failed to get tx report from firmware
[  +7.039651] rtw_8723de 0000:02:00.0: timed out to flush queue 1
[  +3.391807] rtw_8723de 0000:02:00.0: failed to get tx report from firmware
[  +9.631529] rtw_8723de 0000:02:00.0: failed to get tx report from firmware
[  +9.919658] rtw_8723de 0000:02:00.0: failed to get tx report from firmware
[Apr25 16:33] rtw_8723de 0000:02:00.0: failed to get tx report from firmware
[  +9.983882] rtw_8723de 0000:02:00.0: failed to get tx report from firmware
[ +10.015833] rtw_8723de 0000:02:00.0: failed to get tx report from firmware
[ +10.015932] rtw_8723de 0000:02:00.0: failed to get tx report from firmware
[ +10.019969] rtw_8723de 0000:02:00.0: failed to get tx report from firmware
[  +9.980084] rtw_8723de 0000:02:00.0: failed to get tx report from firmware
[Apr25 16:34] rtw_8723de 0000:02:00.0: failed to get tx report from firmware
[  +9.952008] rtw_8723de 0000:02:00.0: failed to get tx report from firmware
[  +9.980119] rtw_8723de 0000:02:00.0: failed to get tx report from firmware
[ +10.080193] rtw_8723de 0000:02:00.0: failed to get tx report from firmware
[  +9.952466] rtw_8723de 0000:02:00.0: failed to get tx report from firmware
[ +10.020833] rtw_8723de 0000:02:00.0: failed to get tx report from firmware


==> etcd [14080f7fbc40] <==
{"level":"warn","ts":"2024-04-25T16:19:06.715428Z","caller":"embed/config.go:679","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2024-04-25T16:19:06.717539Z","caller":"etcdmain/etcd.go:73","msg":"Running: ","args":["etcd","--advertise-client-urls=https://172.17.0.2:2379","--cert-file=/var/lib/minikube/certs/etcd/server.crt","--client-cert-auth=true","--data-dir=/var/lib/minikube/etcd","--experimental-initial-corrupt-check=true","--experimental-watch-progress-notify-interval=5s","--initial-advertise-peer-urls=https://172.17.0.2:2380","--initial-cluster=minikube=https://172.17.0.2:2380","--key-file=/var/lib/minikube/certs/etcd/server.key","--listen-client-urls=https://127.0.0.1:2379,https://172.17.0.2:2379","--listen-metrics-urls=http://127.0.0.1:2381","--listen-peer-urls=https://172.17.0.2:2380","--name=minikube","--peer-cert-file=/var/lib/minikube/certs/etcd/peer.crt","--peer-client-cert-auth=true","--peer-key-file=/var/lib/minikube/certs/etcd/peer.key","--peer-trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt","--proxy-refresh-interval=70000","--snapshot-count=10000","--trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt"]}
{"level":"info","ts":"2024-04-25T16:19:06.717657Z","caller":"etcdmain/etcd.go:116","msg":"server has been already initialized","data-dir":"/var/lib/minikube/etcd","dir-type":"member"}
{"level":"warn","ts":"2024-04-25T16:19:06.717695Z","caller":"embed/config.go:679","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2024-04-25T16:19:06.71771Z","caller":"embed/etcd.go:127","msg":"configuring peer listeners","listen-peer-urls":["https://172.17.0.2:2380"]}
{"level":"info","ts":"2024-04-25T16:19:06.717749Z","caller":"embed/etcd.go:494","msg":"starting with peer TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/peer.crt, key = /var/lib/minikube/certs/etcd/peer.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2024-04-25T16:19:06.722583Z","caller":"embed/etcd.go:135","msg":"configuring client listeners","listen-client-urls":["https://127.0.0.1:2379","https://172.17.0.2:2379"]}
{"level":"info","ts":"2024-04-25T16:19:06.723056Z","caller":"embed/etcd.go:308","msg":"starting an etcd server","etcd-version":"3.5.12","git-sha":"e7b3bb6cc","go-version":"go1.20.13","go-os":"linux","go-arch":"amd64","max-cpu-set":4,"max-cpu-available":4,"member-initialized":true,"name":"minikube","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"max-wals":5,"max-snapshots":5,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://172.17.0.2:2380"],"listen-peer-urls":["https://172.17.0.2:2380"],"advertise-client-urls":["https://172.17.0.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://172.17.0.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"","initial-cluster-state":"new","initial-cluster-token":"","quota-backend-bytes":2147483648,"max-request-bytes":1572864,"max-concurrent-streams":4294967295,"pre-vote":true,"initial-corrupt-check":true,"corrupt-check-time-interval":"0s","compact-check-time-enabled":false,"compact-check-time-interval":"1m0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2024-04-25T16:19:06.741112Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"17.316183ms"}
{"level":"info","ts":"2024-04-25T16:19:06.81795Z","caller":"etcdserver/server.go:532","msg":"No snapshot found. Recovering WAL from scratch!"}
{"level":"info","ts":"2024-04-25T16:19:06.853924Z","caller":"etcdserver/raft.go:530","msg":"restarting local member","cluster-id":"38b0e74a458e7a1f","local-member-id":"b8e14bda2255bc24","commit-index":5361}
{"level":"info","ts":"2024-04-25T16:19:06.855234Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b8e14bda2255bc24 switched to configuration voters=()"}
{"level":"info","ts":"2024-04-25T16:19:06.855964Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b8e14bda2255bc24 became follower at term 2"}
{"level":"info","ts":"2024-04-25T16:19:06.856252Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft b8e14bda2255bc24 [peers: [], term: 2, commit: 5361, applied: 0, lastindex: 5361, lastterm: 2]"}
{"level":"warn","ts":"2024-04-25T16:19:06.859545Z","caller":"auth/store.go:1241","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2024-04-25T16:19:06.865317Z","caller":"mvcc/kvstore.go:341","msg":"restored last compact revision","meta-bucket-name":"meta","meta-bucket-name-key":"finishedCompactRev","restored-compact-revision":4076}
{"level":"info","ts":"2024-04-25T16:19:06.86838Z","caller":"mvcc/kvstore.go:407","msg":"kvstore restored","current-rev":4456}
{"level":"info","ts":"2024-04-25T16:19:06.870553Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2024-04-25T16:19:06.874345Z","caller":"etcdserver/corrupt.go:96","msg":"starting initial corruption check","local-member-id":"b8e14bda2255bc24","timeout":"7s"}
{"level":"info","ts":"2024-04-25T16:19:06.875051Z","caller":"etcdserver/corrupt.go:177","msg":"initial corruption checking passed; no corruption","local-member-id":"b8e14bda2255bc24"}
{"level":"info","ts":"2024-04-25T16:19:06.875292Z","caller":"etcdserver/server.go:860","msg":"starting etcd server","local-member-id":"b8e14bda2255bc24","local-server-version":"3.5.12","cluster-version":"to_be_decided"}
{"level":"info","ts":"2024-04-25T16:19:06.876815Z","caller":"etcdserver/server.go:760","msg":"starting initial election tick advance","election-ticks":10}
{"level":"info","ts":"2024-04-25T16:19:06.877655Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2024-04-25T16:19:06.877785Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2024-04-25T16:19:06.877834Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2024-04-25T16:19:06.878345Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b8e14bda2255bc24 switched to configuration voters=(13322012572989635620)"}
{"level":"info","ts":"2024-04-25T16:19:06.878507Z","caller":"membership/cluster.go:421","msg":"added member","cluster-id":"38b0e74a458e7a1f","local-member-id":"b8e14bda2255bc24","added-peer-id":"b8e14bda2255bc24","added-peer-peer-urls":["https://172.17.0.2:2380"]}
{"level":"info","ts":"2024-04-25T16:19:06.87874Z","caller":"membership/cluster.go:584","msg":"set initial cluster version","cluster-id":"38b0e74a458e7a1f","local-member-id":"b8e14bda2255bc24","cluster-version":"3.5"}
{"level":"info","ts":"2024-04-25T16:19:06.87894Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2024-04-25T16:19:06.885248Z","caller":"embed/etcd.go:726","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2024-04-25T16:19:06.885623Z","caller":"embed/etcd.go:277","msg":"now serving peer/client/metrics","local-member-id":"b8e14bda2255bc24","initial-advertise-peer-urls":["https://172.17.0.2:2380"],"listen-peer-urls":["https://172.17.0.2:2380"],"advertise-client-urls":["https://172.17.0.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://172.17.0.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2024-04-25T16:19:06.885681Z","caller":"embed/etcd.go:857","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2024-04-25T16:19:06.886202Z","caller":"embed/etcd.go:597","msg":"serving peer traffic","address":"172.17.0.2:2380"}
{"level":"info","ts":"2024-04-25T16:19:06.886232Z","caller":"embed/etcd.go:569","msg":"cmux::serve","address":"172.17.0.2:2380"}
{"level":"info","ts":"2024-04-25T16:19:07.857833Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b8e14bda2255bc24 is starting a new election at term 2"}
{"level":"info","ts":"2024-04-25T16:19:07.857993Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b8e14bda2255bc24 became pre-candidate at term 2"}
{"level":"info","ts":"2024-04-25T16:19:07.858163Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b8e14bda2255bc24 received MsgPreVoteResp from b8e14bda2255bc24 at term 2"}
{"level":"info","ts":"2024-04-25T16:19:07.858256Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b8e14bda2255bc24 became candidate at term 3"}
{"level":"info","ts":"2024-04-25T16:19:07.858306Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b8e14bda2255bc24 received MsgVoteResp from b8e14bda2255bc24 at term 3"}
{"level":"info","ts":"2024-04-25T16:19:07.858386Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b8e14bda2255bc24 became leader at term 3"}
{"level":"info","ts":"2024-04-25T16:19:07.858458Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: b8e14bda2255bc24 elected leader b8e14bda2255bc24 at term 3"}
{"level":"info","ts":"2024-04-25T16:19:07.862335Z","caller":"etcdserver/server.go:2068","msg":"published local member to cluster through raft","local-member-id":"b8e14bda2255bc24","local-member-attributes":"{Name:minikube ClientURLs:[https://172.17.0.2:2379]}","request-path":"/0/members/b8e14bda2255bc24/attributes","cluster-id":"38b0e74a458e7a1f","publish-timeout":"7s"}
{"level":"info","ts":"2024-04-25T16:19:07.862725Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-04-25T16:19:07.863139Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-04-25T16:19:07.866499Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"172.17.0.2:2379"}
{"level":"info","ts":"2024-04-25T16:19:07.869914Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2024-04-25T16:19:07.870321Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2024-04-25T16:19:07.871616Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2024-04-25T16:29:08.927235Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":4852}
{"level":"info","ts":"2024-04-25T16:29:08.961002Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":4852,"took":"32.340254ms","hash":2271445810,"current-db-size-bytes":3338240,"current-db-size":"3.3 MB","current-db-size-in-use-bytes":1921024,"current-db-size-in-use":"1.9 MB"}
{"level":"info","ts":"2024-04-25T16:29:08.961191Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2271445810,"revision":4852,"compact-revision":4076}
{"level":"info","ts":"2024-04-25T16:34:08.947869Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":5166}
{"level":"info","ts":"2024-04-25T16:34:08.959218Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":5166,"took":"10.373255ms","hash":2771314043,"current-db-size-bytes":3338240,"current-db-size":"3.3 MB","current-db-size-in-use-bytes":2347008,"current-db-size-in-use":"2.3 MB"}
{"level":"info","ts":"2024-04-25T16:34:08.959363Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2771314043,"revision":5166,"compact-revision":4852}


==> etcd [d1d8d5895bf9] <==
{"level":"info","ts":"2024-04-24T10:09:07.593404Z","caller":"etcdserver/server.go:2578","msg":"setting up initial cluster version using v2 API","cluster-version":"3.5"}
{"level":"info","ts":"2024-04-24T10:09:07.594518Z","caller":"etcdserver/server.go:2068","msg":"published local member to cluster through raft","local-member-id":"b8e14bda2255bc24","local-member-attributes":"{Name:minikube ClientURLs:[https://172.17.0.2:2379]}","request-path":"/0/members/b8e14bda2255bc24/attributes","cluster-id":"38b0e74a458e7a1f","publish-timeout":"7s"}
{"level":"info","ts":"2024-04-24T10:09:07.594682Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-04-24T10:09:07.595836Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-04-24T10:09:07.598902Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2024-04-24T10:09:07.599072Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"172.17.0.2:2379"}
{"level":"info","ts":"2024-04-24T10:09:07.599629Z","caller":"membership/cluster.go:584","msg":"set initial cluster version","cluster-id":"38b0e74a458e7a1f","local-member-id":"b8e14bda2255bc24","cluster-version":"3.5"}
{"level":"info","ts":"2024-04-24T10:09:07.600015Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2024-04-24T10:09:07.600059Z","caller":"etcdserver/server.go:2602","msg":"cluster version is updated","cluster-version":"3.5"}
{"level":"info","ts":"2024-04-24T10:09:07.600123Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2024-04-24T10:09:07.60014Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2024-04-24T10:19:07.651056Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":605}
{"level":"info","ts":"2024-04-24T10:19:07.656543Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":605,"took":"5.124318ms","hash":2211293331,"current-db-size-bytes":1306624,"current-db-size":"1.3 MB","current-db-size-in-use-bytes":1306624,"current-db-size-in-use":"1.3 MB"}
{"level":"info","ts":"2024-04-24T10:19:07.656577Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2211293331,"revision":605,"compact-revision":-1}
{"level":"info","ts":"2024-04-24T10:24:07.664276Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":844}
{"level":"info","ts":"2024-04-24T10:24:07.668251Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":844,"took":"3.495009ms","hash":660516194,"current-db-size-bytes":1306624,"current-db-size":"1.3 MB","current-db-size-in-use-bytes":1060864,"current-db-size-in-use":"1.1 MB"}
{"level":"info","ts":"2024-04-24T10:24:07.668359Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":660516194,"revision":844,"compact-revision":605}
{"level":"info","ts":"2024-04-24T10:29:07.680257Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":1141}
{"level":"info","ts":"2024-04-24T10:29:07.688406Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":1141,"took":"7.416192ms","hash":2896622946,"current-db-size-bytes":1339392,"current-db-size":"1.3 MB","current-db-size-in-use-bytes":1339392,"current-db-size-in-use":"1.3 MB"}
{"level":"info","ts":"2024-04-24T10:29:07.688499Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2896622946,"revision":1141,"compact-revision":844}
{"level":"info","ts":"2024-04-24T10:34:07.695135Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":1433}
{"level":"info","ts":"2024-04-24T10:34:07.698149Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":1433,"took":"2.76319ms","hash":1243219185,"current-db-size-bytes":1339392,"current-db-size":"1.3 MB","current-db-size-in-use-bytes":1257472,"current-db-size-in-use":"1.3 MB"}
{"level":"info","ts":"2024-04-24T10:34:07.69818Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1243219185,"revision":1433,"compact-revision":1141}
{"level":"info","ts":"2024-04-24T10:39:07.707634Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":1680}
{"level":"info","ts":"2024-04-24T10:39:07.713202Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":1680,"took":"5.022259ms","hash":2636226765,"current-db-size-bytes":1425408,"current-db-size":"1.4 MB","current-db-size-in-use-bytes":1425408,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2024-04-24T10:39:07.713276Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2636226765,"revision":1680,"compact-revision":1433}
{"level":"info","ts":"2024-04-24T10:44:07.722416Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":2046}
{"level":"info","ts":"2024-04-24T10:44:07.735953Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":2046,"took":"13.10821ms","hash":1376932027,"current-db-size-bytes":1794048,"current-db-size":"1.8 MB","current-db-size-in-use-bytes":1794048,"current-db-size-in-use":"1.8 MB"}
{"level":"info","ts":"2024-04-24T10:44:07.736011Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1376932027,"revision":2046,"compact-revision":1680}
{"level":"info","ts":"2024-04-24T10:49:07.739933Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":2378}
{"level":"info","ts":"2024-04-24T10:49:07.750861Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":2378,"took":"8.661681ms","hash":968397712,"current-db-size-bytes":1794048,"current-db-size":"1.8 MB","current-db-size-in-use-bytes":1421312,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2024-04-24T10:49:07.750975Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":968397712,"revision":2378,"compact-revision":2046}
{"level":"info","ts":"2024-04-24T10:54:07.752318Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":2625}
{"level":"info","ts":"2024-04-24T10:54:07.760204Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":2625,"took":"6.859106ms","hash":69909404,"current-db-size-bytes":1794048,"current-db-size":"1.8 MB","current-db-size-in-use-bytes":1085440,"current-db-size-in-use":"1.1 MB"}
{"level":"info","ts":"2024-04-24T10:54:07.760356Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":69909404,"revision":2625,"compact-revision":2378}
{"level":"info","ts":"2024-04-24T10:59:07.761451Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":2873}
{"level":"info","ts":"2024-04-24T10:59:07.766127Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":2873,"took":"4.188106ms","hash":664689982,"current-db-size-bytes":1794048,"current-db-size":"1.8 MB","current-db-size-in-use-bytes":1069056,"current-db-size-in-use":"1.1 MB"}
{"level":"info","ts":"2024-04-24T10:59:07.766217Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":664689982,"revision":2873,"compact-revision":2625}
{"level":"info","ts":"2024-04-24T11:04:07.772313Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":3119}
{"level":"info","ts":"2024-04-24T11:04:07.778322Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":3119,"took":"4.850765ms","hash":2412614912,"current-db-size-bytes":1916928,"current-db-size":"1.9 MB","current-db-size-in-use-bytes":1490944,"current-db-size-in-use":"1.5 MB"}
{"level":"info","ts":"2024-04-24T11:04:07.778406Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2412614912,"revision":3119,"compact-revision":2873}
{"level":"info","ts":"2024-04-24T11:09:07.788778Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":3425}
{"level":"info","ts":"2024-04-24T11:09:07.7982Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":3425,"took":"8.60428ms","hash":975947148,"current-db-size-bytes":2060288,"current-db-size":"2.1 MB","current-db-size-in-use-bytes":2060288,"current-db-size-in-use":"2.1 MB"}
{"level":"info","ts":"2024-04-24T11:09:07.798307Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":975947148,"revision":3425,"compact-revision":3119}
{"level":"info","ts":"2024-04-24T11:14:07.804707Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":3788}
{"level":"info","ts":"2024-04-24T11:14:07.808763Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":3788,"took":"3.399741ms","hash":141850998,"current-db-size-bytes":2060288,"current-db-size":"2.1 MB","current-db-size-in-use-bytes":1789952,"current-db-size-in-use":"1.8 MB"}
{"level":"info","ts":"2024-04-24T11:14:07.808812Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":141850998,"revision":3788,"compact-revision":3425}
{"level":"info","ts":"2024-04-24T11:19:07.82Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":4076}
{"level":"info","ts":"2024-04-24T11:19:07.829384Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":4076,"took":"7.711469ms","hash":284234265,"current-db-size-bytes":2060288,"current-db-size":"2.1 MB","current-db-size-in-use-bytes":1798144,"current-db-size-in-use":"1.8 MB"}
{"level":"info","ts":"2024-04-24T11:19:07.829523Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":284234265,"revision":4076,"compact-revision":3788}
{"level":"info","ts":"2024-04-24T11:19:34.594354Z","caller":"osutil/interrupt_unix.go:64","msg":"received signal; shutting down","signal":"terminated"}
{"level":"info","ts":"2024-04-24T11:19:34.594421Z","caller":"embed/etcd.go:375","msg":"closing etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://172.17.0.2:2380"],"advertise-client-urls":["https://172.17.0.2:2379"]}
{"level":"warn","ts":"2024-04-24T11:19:34.594516Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2024-04-24T11:19:34.594607Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2024-04-24T11:19:34.674176Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 172.17.0.2:2379: use of closed network connection"}
{"level":"warn","ts":"2024-04-24T11:19:34.674227Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 172.17.0.2:2379: use of closed network connection"}
{"level":"info","ts":"2024-04-24T11:19:34.674296Z","caller":"etcdserver/server.go:1471","msg":"skipped leadership transfer for single voting member cluster","local-member-id":"b8e14bda2255bc24","current-leader-member-id":"b8e14bda2255bc24"}
{"level":"info","ts":"2024-04-24T11:19:34.684041Z","caller":"embed/etcd.go:579","msg":"stopping serving peer traffic","address":"172.17.0.2:2380"}
{"level":"info","ts":"2024-04-24T11:19:34.6842Z","caller":"embed/etcd.go:584","msg":"stopped serving peer traffic","address":"172.17.0.2:2380"}
{"level":"info","ts":"2024-04-24T11:19:34.684217Z","caller":"embed/etcd.go:377","msg":"closed etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://172.17.0.2:2380"],"advertise-client-urls":["https://172.17.0.2:2379"]}


==> kernel <==
 16:35:04 up 12:58,  0 users,  load average: 1.49, 1.74, 1.62
Linux minikube 6.5.0-28-generic #29~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Apr  4 14:39:20 UTC 2 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.4 LTS"


==> kube-apiserver [5a3623220411] <==
I0425 16:19:09.470643       1 secure_serving.go:213] Serving securely on [::]:8443
I0425 16:19:09.470725       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0425 16:19:09.470843       1 apiservice_controller.go:97] Starting APIServiceRegistrationController
I0425 16:19:09.471007       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I0425 16:19:09.471121       1 customresource_discovery_controller.go:289] Starting DiscoveryController
I0425 16:19:09.471230       1 available_controller.go:423] Starting AvailableConditionController
I0425 16:19:09.471249       1 cache.go:32] Waiting for caches to sync for AvailableConditionController controller
I0425 16:19:09.471370       1 controller.go:80] Starting OpenAPI V3 AggregationController
I0425 16:19:09.471422       1 gc_controller.go:78] Starting apiserver lease garbage collector
I0425 16:19:09.471491       1 controller.go:139] Starting OpenAPI controller
I0425 16:19:09.471527       1 controller.go:87] Starting OpenAPI V3 controller
I0425 16:19:09.471548       1 naming_controller.go:291] Starting NamingConditionController
I0425 16:19:09.471572       1 establishing_controller.go:76] Starting EstablishingController
I0425 16:19:09.471629       1 controller.go:116] Starting legacy_token_tracking_controller
I0425 16:19:09.471641       1 nonstructuralschema_controller.go:192] Starting NonStructuralSchemaConditionController
I0425 16:19:09.471643       1 shared_informer.go:313] Waiting for caches to sync for configmaps
I0425 16:19:09.471658       1 apiapproval_controller.go:186] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0425 16:19:09.471678       1 crd_finalizer.go:266] Starting CRDFinalizer
I0425 16:19:09.471707       1 controller.go:78] Starting OpenAPI AggregationController
I0425 16:19:09.472115       1 dynamic_serving_content.go:132] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I0425 16:19:09.472305       1 apf_controller.go:374] Starting API Priority and Fairness config controller
I0425 16:19:09.472465       1 cluster_authentication_trust_controller.go:440] Starting cluster_authentication_trust_controller controller
I0425 16:19:09.472625       1 shared_informer.go:313] Waiting for caches to sync for cluster_authentication_trust_controller
I0425 16:19:09.472978       1 system_namespaces_controller.go:67] Starting system namespaces controller
I0425 16:19:09.473143       1 aggregator.go:163] waiting for initial CRD sync...
I0425 16:19:09.473986       1 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0425 16:19:09.474340       1 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0425 16:19:09.474858       1 crdregistration_controller.go:111] Starting crd-autoregister controller
I0425 16:19:09.475032       1 shared_informer.go:313] Waiting for caches to sync for crd-autoregister
I0425 16:19:09.571293       1 cache.go:39] Caches are synced for AvailableConditionController controller
I0425 16:19:09.571997       1 shared_informer.go:320] Caches are synced for configmaps
I0425 16:19:09.572063       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0425 16:19:09.572823       1 apf_controller.go:379] Running API Priority and Fairness config worker
I0425 16:19:09.573046       1 apf_controller.go:382] Running API Priority and Fairness periodic rebalancing process
I0425 16:19:09.573821       1 shared_informer.go:320] Caches are synced for cluster_authentication_trust_controller
I0425 16:19:09.578502       1 shared_informer.go:320] Caches are synced for crd-autoregister
I0425 16:19:09.578546       1 aggregator.go:165] initial CRD sync complete...
I0425 16:19:09.578555       1 autoregister_controller.go:141] Starting autoregister controller
I0425 16:19:09.578563       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0425 16:19:09.578571       1 cache.go:39] Caches are synced for autoregister controller
I0425 16:19:09.599644       1 handler_discovery.go:447] Starting ResourceDiscoveryManager
E0425 16:19:09.601927       1 controller.go:97] Error removing old endpoints from kubernetes service: no API server IP addresses were listed in storage, refusing to erase all endpoints for the kubernetes Service
I0425 16:19:09.606228       1 shared_informer.go:320] Caches are synced for node_authorizer
I0425 16:19:09.609976       1 shared_informer.go:320] Caches are synced for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]
I0425 16:19:09.610156       1 policy_source.go:224] refreshing policies
I0425 16:19:09.624146       1 controller.go:615] quota admission added evaluator for: leases.coordination.k8s.io
I0425 16:19:10.178832       1 controller.go:615] quota admission added evaluator for: endpoints
I0425 16:19:10.476289       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
E0425 16:19:19.639082       1 controller.go:195] "Failed to update lease" err="Operation cannot be fulfilled on leases.coordination.k8s.io \"apiserver-eqt674mfxb4j56mrjjkoe7b7ii\": StorageError: invalid object, Code: 4, Key: /registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii, ResourceVersion: 0, AdditionalErrorMsg: Precondition failed: UID in precondition: 71518d2e-6d9e-4119-b039-631f5c1626cd, UID in object meta: "
I0425 16:19:22.805391       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0425 16:26:02.088247       1 controller.go:615] quota admission added evaluator for: statefulsets.apps
I0425 16:26:10.715452       1 controller.go:615] quota admission added evaluator for: deployments.apps
I0425 16:28:24.102271       1 controller.go:615] quota admission added evaluator for: namespaces
I0425 16:28:24.114733       1 controller.go:615] quota admission added evaluator for: serviceaccounts
I0425 16:28:24.133632       1 controller.go:615] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I0425 16:28:24.168219       1 controller.go:615] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I0425 16:28:24.243108       1 alloc.go:330] "allocated clusterIPs" service="ingress-nginx/ingress-nginx-controller" clusterIPs={"IPv4":"10.98.107.223"}
I0425 16:28:24.264036       1 alloc.go:330] "allocated clusterIPs" service="ingress-nginx/ingress-nginx-controller-admission" clusterIPs={"IPv4":"10.105.128.127"}
I0425 16:28:24.285966       1 controller.go:615] quota admission added evaluator for: replicasets.apps
I0425 16:28:24.289590       1 controller.go:615] quota admission added evaluator for: jobs.batch


==> kube-apiserver [71aa92278060] <==
W0424 11:19:39.959785       1 logging.go:59] [core] [Channel #55 SubChannel #56] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0424 11:19:39.961655       1 logging.go:59] [core] [Channel #103 SubChannel #104] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0424 11:19:39.997697       1 logging.go:59] [core] [Channel #79 SubChannel #80] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0424 11:19:40.016773       1 logging.go:59] [core] [Channel #70 SubChannel #71] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0424 11:19:40.026661       1 logging.go:59] [core] [Channel #94 SubChannel #95] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0424 11:19:40.026662       1 logging.go:59] [core] [Channel #151 SubChannel #152] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0424 11:19:40.046812       1 logging.go:59] [core] [Channel #160 SubChannel #161] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0424 11:19:40.096963       1 logging.go:59] [core] [Channel #115 SubChannel #116] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0424 11:19:40.104285       1 logging.go:59] [core] [Channel #127 SubChannel #128] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0424 11:19:40.110772       1 logging.go:59] [core] [Channel #64 SubChannel #65] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0424 11:19:40.126403       1 logging.go:59] [core] [Channel #58 SubChannel #59] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0424 11:19:40.127179       1 logging.go:59] [core] [Channel #181 SubChannel #182] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0424 11:19:40.159157       1 logging.go:59] [core] [Channel #97 SubChannel #98] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0424 11:19:40.203491       1 logging.go:59] [core] [Channel #148 SubChannel #149] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0424 11:19:40.207534       1 logging.go:59] [core] [Channel #19 SubChannel #20] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0424 11:19:40.211693       1 logging.go:59] [core] [Channel #163 SubChannel #164] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0424 11:19:40.301530       1 logging.go:59] [core] [Channel #154 SubChannel #155] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0424 11:19:40.445010       1 logging.go:59] [core] [Channel #106 SubChannel #107] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0424 11:19:42.770071       1 logging.go:59] [core] [Channel #49 SubChannel #50] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0424 11:19:42.996482       1 logging.go:59] [core] [Channel #145 SubChannel #146] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0424 11:19:43.041811       1 logging.go:59] [core] [Channel #37 SubChannel #38] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0424 11:19:43.059526       1 logging.go:59] [core] [Channel #100 SubChannel #101] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0424 11:19:43.227412       1 logging.go:59] [core] [Channel #61 SubChannel #62] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0424 11:19:43.250825       1 logging.go:59] [core] [Channel #52 SubChannel #53] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0424 11:19:43.316710       1 logging.go:59] [core] [Channel #91 SubChannel #92] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0424 11:19:43.334727       1 logging.go:59] [core] [Channel #73 SubChannel #74] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0424 11:19:43.337536       1 logging.go:59] [core] [Channel #10 SubChannel #11] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0424 11:19:43.339385       1 logging.go:59] [core] [Channel #133 SubChannel #134] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0424 11:19:43.358187       1 logging.go:59] [core] [Channel #70 SubChannel #71] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0424 11:19:43.389408       1 logging.go:59] [core] [Channel #166 SubChannel #167] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0424 11:19:43.402063       1 logging.go:59] [core] [Channel #88 SubChannel #89] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0424 11:19:43.429210       1 logging.go:59] [core] [Channel #43 SubChannel #44] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0424 11:19:43.540555       1 logging.go:59] [core] [Channel #157 SubChannel #158] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0424 11:19:43.548761       1 logging.go:59] [core] [Channel #79 SubChannel #80] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0424 11:19:43.581117       1 logging.go:59] [core] [Channel #172 SubChannel #173] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0424 11:19:43.586404       1 logging.go:59] [core] [Channel #85 SubChannel #86] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0424 11:19:43.595000       1 logging.go:59] [core] [Channel #121 SubChannel #122] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0424 11:19:43.630364       1 logging.go:59] [core] [Channel #67 SubChannel #68] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0424 11:19:43.636426       1 logging.go:59] [core] [Channel #19 SubChannel #20] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0424 11:19:43.650634       1 logging.go:59] [core] [Channel #25 SubChannel #26] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0424 11:19:43.654578       1 logging.go:59] [core] [Channel #151 SubChannel #152] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0424 11:19:43.654664       1 logging.go:59] [core] [Channel #22 SubChannel #23] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0424 11:19:43.691083       1 logging.go:59] [core] [Channel #148 SubChannel #149] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0424 11:19:43.716845       1 logging.go:59] [core] [Channel #76 SubChannel #77] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0424 11:19:43.736712       1 logging.go:59] [core] [Channel #97 SubChannel #98] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0424 11:19:43.739623       1 logging.go:59] [core] [Channel #3 SubChannel #4] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0424 11:19:43.740170       1 logging.go:59] [core] [Channel #13 SubChannel #15] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0424 11:19:43.762756       1 logging.go:59] [core] [Channel #82 SubChannel #83] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0424 11:19:43.785528       1 logging.go:59] [core] [Channel #1 SubChannel #2] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0424 11:19:43.841781       1 logging.go:59] [core] [Channel #112 SubChannel #113] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0424 11:19:43.886431       1 logging.go:59] [core] [Channel #136 SubChannel #137] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0424 11:19:43.955534       1 logging.go:59] [core] [Channel #169 SubChannel #170] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0424 11:19:43.974314       1 logging.go:59] [core] [Channel #109 SubChannel #110] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0424 11:19:43.979207       1 logging.go:59] [core] [Channel #175 SubChannel #176] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0424 11:19:43.987875       1 logging.go:59] [core] [Channel #139 SubChannel #140] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0424 11:19:44.013314       1 logging.go:59] [core] [Channel #130 SubChannel #131] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0424 11:19:44.079441       1 logging.go:59] [core] [Channel #178 SubChannel #179] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0424 11:19:44.092885       1 logging.go:59] [core] [Channel #160 SubChannel #161] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0424 11:19:44.094608       1 logging.go:59] [core] [Channel #115 SubChannel #116] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0424 11:19:44.136624       1 logging.go:59] [core] [Channel #181 SubChannel #182] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"


==> kube-controller-manager [2ac223c654ad] <==
I0425 16:19:34.340927       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/blog-web-66455df769" duration="10.8363ms"
I0425 16:19:34.341027       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/blog-web-66455df769" duration="52.922¬µs"
I0425 16:19:45.089730       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/blog-web-66455df769" duration="52.284¬µs"
I0425 16:19:45.669356       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/blog-web-66455df769" duration="10.066567ms"
I0425 16:19:45.669636       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/blog-web-66455df769" duration="98.123¬µs"
I0425 16:19:50.931139       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7db6d8ff4d" duration="10.457082ms"
I0425 16:19:50.931258       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7db6d8ff4d" duration="70.852¬µs"
I0425 16:19:55.903858       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/blog-web-66455df769" duration="22.988685ms"
I0425 16:19:55.904531       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/blog-web-66455df769" duration="161.955¬µs"
I0425 16:20:08.089132       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/blog-web-66455df769" duration="49.77¬µs"
I0425 16:20:21.505496       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/blog-web-66455df769" duration="9.253313ms"
I0425 16:20:21.505706       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/blog-web-66455df769" duration="55.169¬µs"
I0425 16:20:26.588032       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/blog-web-66455df769" duration="11.14836ms"
I0425 16:20:26.588124       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/blog-web-66455df769" duration="48.484¬µs"
I0425 16:20:40.114635       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/blog-web-66455df769" duration="158.824¬µs"
I0425 16:21:07.604139       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/blog-web-66455df769" duration="9.668573ms"
I0425 16:21:07.604497       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/blog-web-66455df769" duration="56.566¬µs"
I0425 16:21:12.748443       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/blog-web-66455df769" duration="8.36329ms"
I0425 16:21:12.748555       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/blog-web-66455df769" duration="51.693¬µs"
I0425 16:21:27.089662       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/blog-web-66455df769" duration="50.026¬µs"
I0425 16:22:46.174178       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/blog-web-66455df769" duration="14.478796ms"
I0425 16:22:46.174617       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/blog-web-66455df769" duration="52.741¬µs"
I0425 16:22:52.329417       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/blog-web-66455df769" duration="20.26889ms"
I0425 16:22:52.329817       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/blog-web-66455df769" duration="91.831¬µs"
I0425 16:23:08.094651       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/blog-web-66455df769" duration="141.209¬µs"
I0425 16:25:40.703112       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/blog-web-66455df769" duration="14.031462ms"
I0425 16:25:40.703539       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/blog-web-66455df769" duration="192.847¬µs"
I0425 16:25:45.842736       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/blog-web-66455df769" duration="16.547216ms"
I0425 16:25:45.842848       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/blog-web-66455df769" duration="44.699¬µs"
I0425 16:26:00.103214       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/blog-web-66455df769" duration="94.622¬µs"
I0425 16:28:24.295472       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create"
I0425 16:28:24.338721       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create"
I0425 16:28:24.345310       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-84df5799c" duration="54.687864ms"
I0425 16:28:24.349552       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch"
I0425 16:28:24.357974       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create"
I0425 16:28:24.361213       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create"
I0425 16:28:24.364517       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch"
I0425 16:28:24.379703       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch"
I0425 16:28:24.379750       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch"
I0425 16:28:24.390982       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-84df5799c" duration="45.536784ms"
I0425 16:28:24.391106       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-84df5799c" duration="76.951¬µs"
I0425 16:28:24.406248       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create"
I0425 16:28:24.417286       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch"
I0425 16:30:56.756762       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/blog-web-66455df769" duration="13.716764ms"
I0425 16:30:56.756859       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/blog-web-66455df769" duration="55.501¬µs"
I0425 16:31:01.878035       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/blog-web-66455df769" duration="11.297663ms"
I0425 16:31:01.879811       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/blog-web-66455df769" duration="48.886¬µs"
I0425 16:31:17.103602       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/blog-web-66455df769" duration="98.742¬µs"
I0425 16:32:13.649708       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create"
I0425 16:32:14.739135       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch"
I0425 16:32:15.050204       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create"
I0425 16:32:15.837127       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create"
I0425 16:32:16.064419       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create"
I0425 16:32:16.079718       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create"
I0425 16:32:16.088911       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create"
I0425 16:32:16.102118       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch"
I0425 16:32:16.912347       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch"
I0425 16:32:17.126604       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch"
I0425 16:32:17.156160       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch"
I0425 16:32:17.175967       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch"


==> kube-controller-manager [fb77d699ecd7] <==
I0424 11:08:47.194635       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/blog-web-66455df769" duration="11.328188ms"
I0424 11:08:47.194729       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/blog-web-66455df769" duration="37.977¬µs"
I0424 11:08:59.284148       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/blog-web-66455df769" duration="63.678¬µs"
I0424 11:09:14.887755       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/blog-web-66455df769" duration="16.889213ms"
I0424 11:09:14.888208       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/blog-web-66455df769" duration="162.308¬µs"
I0424 11:09:19.977527       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/blog-web-66455df769" duration="12.52883ms"
I0424 11:09:19.977878       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/blog-web-66455df769" duration="55.898¬µs"
I0424 11:09:32.301358       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/blog-web-66455df769" duration="144.598¬µs"
I0424 11:10:02.814143       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/blog-web-66455df769" duration="11.956011ms"
I0424 11:10:02.814746       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/blog-web-66455df769" duration="558.348¬µs"
I0424 11:10:07.918103       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/blog-web-66455df769" duration="14.760859ms"
I0424 11:10:07.918209       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/blog-web-66455df769" duration="58.958¬µs"
I0424 11:10:20.311755       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/blog-web-66455df769" duration="145.804¬µs"
I0424 11:11:37.687967       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/blog-web-66455df769" duration="8.462721ms"
I0424 11:11:37.688047       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/blog-web-66455df769" duration="37.864¬µs"
I0424 11:11:41.796584       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/blog-web-66455df769" duration="11.496887ms"
I0424 11:11:41.796672       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/blog-web-66455df769" duration="53.636¬µs"
I0424 11:11:55.282570       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/blog-web-66455df769" duration="67.551¬µs"
I0424 11:14:28.552938       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/blog-web-66455df769" duration="8.964106ms"
I0424 11:14:28.553144       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/blog-web-66455df769" duration="79.611¬µs"
I0424 11:14:33.664213       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/blog-web-66455df769" duration="11.665151ms"
I0424 11:14:33.664307       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/blog-web-66455df769" duration="53.331¬µs"
I0424 11:14:46.318081       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/blog-web-66455df769" duration="166.65¬µs"
I0424 11:15:38.832480       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/blog-web-66455df769" duration="42.588838ms"
I0424 11:15:38.832627       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/blog-web-66455df769" duration="79.871¬µs"
I0424 11:15:38.843333       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/blog-web-66455df769" duration="10.517935ms"
I0424 11:15:38.843454       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/blog-web-66455df769" duration="55.994¬µs"
I0424 11:15:38.850847       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/blog-web-66455df769" duration="50.409¬µs"
I0424 11:15:39.064125       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/blog-web-66455df769" duration="67.355¬µs"
I0424 11:15:39.998238       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/blog-web-66455df769" duration="77.637¬µs"
I0424 11:15:40.040974       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/blog-web-66455df769" duration="24.70383ms"
I0424 11:15:40.041769       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/blog-web-66455df769" duration="67.077¬µs"
I0424 11:15:40.048952       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/blog-web-66455df769" duration="52.601¬µs"
I0424 11:15:44.077635       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/blog-web-66455df769" duration="15.149777ms"
I0424 11:15:44.077749       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/blog-web-66455df769" duration="67.662¬µs"
I0424 11:15:45.090560       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/blog-web-66455df769" duration="7.115803ms"
I0424 11:15:45.090635       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/blog-web-66455df769" duration="39.706¬µs"
I0424 11:15:49.157374       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/blog-web-66455df769" duration="8.729808ms"
I0424 11:15:49.157469       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/blog-web-66455df769" duration="39.661¬µs"
I0424 11:16:03.320759       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/blog-web-66455df769" duration="162.643¬µs"
I0424 11:16:04.628081       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/blog-web-66455df769" duration="21.887111ms"
I0424 11:16:04.628371       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/blog-web-66455df769" duration="161.972¬µs"
I0424 11:16:09.690111       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/blog-web-66455df769" duration="11.9076ms"
I0424 11:16:09.690224       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/blog-web-66455df769" duration="68.583¬µs"
I0424 11:16:23.290703       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/blog-web-66455df769" duration="143.273¬µs"
I0424 11:16:39.245593       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/blog-web-66455df769" duration="11.001452ms"
I0424 11:16:39.245714       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/blog-web-66455df769" duration="56.074¬µs"
I0424 11:16:43.324664       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/blog-web-66455df769" duration="15.985878ms"
I0424 11:16:43.324832       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/blog-web-66455df769" duration="79.595¬µs"
I0424 11:16:54.277734       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/blog-web-66455df769" duration="70.977¬µs"
I0424 11:17:31.925450       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/blog-web-66455df769" duration="11.271007ms"
I0424 11:17:31.925789       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/blog-web-66455df769" duration="79.75¬µs"
I0424 11:17:35.988561       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/blog-web-66455df769" duration="14.117744ms"
I0424 11:17:35.988666       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/blog-web-66455df769" duration="58.848¬µs"
I0424 11:17:47.295308       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/blog-web-66455df769" duration="96.962¬µs"
I0424 11:19:04.606549       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/blog-web-66455df769" duration="9.847283ms"
I0424 11:19:04.606856       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/blog-web-66455df769" duration="44.956¬µs"
I0424 11:19:09.731553       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/blog-web-66455df769" duration="16.352531ms"
I0424 11:19:09.731657       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/blog-web-66455df769" duration="64.809¬µs"
I0424 11:19:21.282631       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/blog-web-66455df769" duration="54.247¬µs"


==> kube-proxy [08d1ea4df626] <==
I0424 10:09:27.273634       1 server_linux.go:69] "Using iptables proxy"
I0424 10:09:27.282696       1 server.go:1062] "Successfully retrieved node IP(s)" IPs=["172.17.0.2"]
I0424 10:09:27.322493       1 server.go:659] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0424 10:09:27.322544       1 server_linux.go:165] "Using iptables Proxier"
I0424 10:09:27.326146       1 server_linux.go:511] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I0424 10:09:27.326179       1 server_linux.go:528] "Defaulting to no-op detect-local"
I0424 10:09:27.326214       1 proxier.go:243] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0424 10:09:27.326562       1 server.go:872] "Version info" version="v1.30.0"
I0424 10:09:27.326589       1 server.go:874] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0424 10:09:27.328566       1 config.go:319] "Starting node config controller"
I0424 10:09:27.328596       1 shared_informer.go:313] Waiting for caches to sync for node config
I0424 10:09:27.329144       1 config.go:101] "Starting endpoint slice config controller"
I0424 10:09:27.329166       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0424 10:09:27.329298       1 config.go:192] "Starting service config controller"
I0424 10:09:27.329313       1 shared_informer.go:313] Waiting for caches to sync for service config
I0424 10:09:27.429079       1 shared_informer.go:320] Caches are synced for node config
I0424 10:09:27.430243       1 shared_informer.go:320] Caches are synced for service config
I0424 10:09:27.430286       1 shared_informer.go:320] Caches are synced for endpoint slice config


==> kube-proxy [ab1a31c5164b] <==
I0425 16:19:12.157765       1 server_linux.go:69] "Using iptables proxy"
I0425 16:19:12.201912       1 server.go:1062] "Successfully retrieved node IP(s)" IPs=["172.17.0.2"]
I0425 16:19:12.307482       1 server.go:659] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0425 16:19:12.308061       1 server_linux.go:165] "Using iptables Proxier"
I0425 16:19:12.312520       1 server_linux.go:511] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I0425 16:19:12.312874       1 server_linux.go:528] "Defaulting to no-op detect-local"
I0425 16:19:12.313850       1 proxier.go:243] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0425 16:19:12.314900       1 server.go:872] "Version info" version="v1.30.0"
I0425 16:19:12.315226       1 server.go:874] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0425 16:19:12.318107       1 config.go:192] "Starting service config controller"
I0425 16:19:12.318586       1 config.go:101] "Starting endpoint slice config controller"
I0425 16:19:12.318910       1 config.go:319] "Starting node config controller"
I0425 16:19:12.319130       1 shared_informer.go:313] Waiting for caches to sync for service config
I0425 16:19:12.319263       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0425 16:19:12.319424       1 shared_informer.go:313] Waiting for caches to sync for node config
I0425 16:19:12.420418       1 shared_informer.go:320] Caches are synced for service config
I0425 16:19:12.426290       1 shared_informer.go:320] Caches are synced for node config
I0425 16:19:12.426331       1 shared_informer.go:320] Caches are synced for endpoint slice config


==> kube-scheduler [a647d31a6975] <==
W0424 10:09:09.104461       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0424 10:09:09.104492       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
W0424 10:09:09.104615       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0424 10:09:09.104643       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
W0424 10:09:09.104723       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0424 10:09:09.104749       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
W0424 10:09:09.106524       1 reflector.go:547] runtime/asm_amd64.s:1695: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0424 10:09:09.106581       1 reflector.go:150] runtime/asm_amd64.s:1695: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
W0424 10:09:09.106880       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0424 10:09:09.106927       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
W0424 10:09:09.107076       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0424 10:09:09.107107       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
W0424 10:09:09.110979       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0424 10:09:09.111028       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
W0424 10:09:09.111259       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0424 10:09:09.111340       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
W0424 10:09:09.111454       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0424 10:09:09.111518       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
W0424 10:09:09.111634       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0424 10:09:09.111696       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W0424 10:09:09.111828       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0424 10:09:09.111891       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
W0424 10:09:09.112004       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0424 10:09:09.112065       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W0424 10:09:09.112254       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0424 10:09:09.112326       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
W0424 10:09:09.112464       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0424 10:09:09.112547       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
W0424 10:09:09.920669       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0424 10:09:09.920776       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
W0424 10:09:10.013799       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0424 10:09:10.014491       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
W0424 10:09:10.035608       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0424 10:09:10.037516       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
W0424 10:09:10.071368       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0424 10:09:10.071529       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
W0424 10:09:10.075471       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0424 10:09:10.075572       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
W0424 10:09:10.080202       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0424 10:09:10.080366       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
W0424 10:09:10.126736       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0424 10:09:10.126774       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W0424 10:09:10.339406       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0424 10:09:10.339441       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W0424 10:09:10.341802       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0424 10:09:10.341847       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
W0424 10:09:10.421319       1 reflector.go:547] runtime/asm_amd64.s:1695: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0424 10:09:10.421421       1 reflector.go:150] runtime/asm_amd64.s:1695: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
W0424 10:09:10.482236       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0424 10:09:10.482363       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
W0424 10:09:10.521766       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0424 10:09:10.522491       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
W0424 10:09:10.523563       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0424 10:09:10.523661       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
W0424 10:09:10.610810       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0424 10:09:10.611740       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
I0424 10:09:12.500941       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0424 11:19:34.715399       1 tlsconfig.go:255] "Shutting down DynamicServingCertificateController"
I0424 11:19:34.715132       1 secure_serving.go:258] Stopped listening on 127.0.0.1:10259
E0424 11:19:34.715170       1 run.go:74] "command failed" err="finished without leader elect"


==> kube-scheduler [a6dc1dacc158] <==
I0425 16:19:07.592080       1 serving.go:380] Generated self-signed cert in-memory
W0425 16:19:09.515663       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0425 16:19:09.515810       1 authentication.go:368] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0425 16:19:09.515891       1 authentication.go:369] Continuing without authentication configuration. This may treat all requests as anonymous.
W0425 16:19:09.515965       1 authentication.go:370] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0425 16:19:09.543020       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.30.0"
I0425 16:19:09.543137       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0425 16:19:09.556420       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0425 16:19:09.556700       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0425 16:19:09.558127       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0425 16:19:09.558457       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0425 16:19:09.657538       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kubelet <==
Apr 25 16:29:54 minikube kubelet[1267]: E0425 16:29:54.072993    1267 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"blog-web\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=blog-web pod=blog-web-66455df769-f6cs8_default(cbb95b54-642f-40cb-84bc-767af2de2b2f)\"" pod="default/blog-web-66455df769-f6cs8" podUID="cbb95b54-642f-40cb-84bc-767af2de2b2f"
Apr 25 16:30:05 minikube kubelet[1267]: I0425 16:30:05.073008    1267 scope.go:117] "RemoveContainer" containerID="a0ceba29cc317f1baa2ceb6d2562e833a3c346cedce9b93c12b0ac597aa853a5"
Apr 25 16:30:05 minikube kubelet[1267]: E0425 16:30:05.074782    1267 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"blog-web\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=blog-web pod=blog-web-66455df769-f6cs8_default(cbb95b54-642f-40cb-84bc-767af2de2b2f)\"" pod="default/blog-web-66455df769-f6cs8" podUID="cbb95b54-642f-40cb-84bc-767af2de2b2f"
Apr 25 16:30:20 minikube kubelet[1267]: I0425 16:30:20.072399    1267 scope.go:117] "RemoveContainer" containerID="a0ceba29cc317f1baa2ceb6d2562e833a3c346cedce9b93c12b0ac597aa853a5"
Apr 25 16:30:20 minikube kubelet[1267]: E0425 16:30:20.072812    1267 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"blog-web\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=blog-web pod=blog-web-66455df769-f6cs8_default(cbb95b54-642f-40cb-84bc-767af2de2b2f)\"" pod="default/blog-web-66455df769-f6cs8" podUID="cbb95b54-642f-40cb-84bc-767af2de2b2f"
Apr 25 16:30:27 minikube kubelet[1267]: E0425 16:30:27.363962    1267 pod_workers.go:1298] "Error syncing pod, skipping" err="unmounted volumes=[webhook-cert], unattached volumes=[], failed to process volumes=[]: context deadline exceeded" pod="ingress-nginx/ingress-nginx-controller-84df5799c-8lrhc" podUID="2dabb154-16be-4272-bc08-641cf3d863ce"
Apr 25 16:30:32 minikube kubelet[1267]: E0425 16:30:32.308690    1267 secret.go:194] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Apr 25 16:30:32 minikube kubelet[1267]: E0425 16:30:32.309323    1267 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/2dabb154-16be-4272-bc08-641cf3d863ce-webhook-cert podName:2dabb154-16be-4272-bc08-641cf3d863ce nodeName:}" failed. No retries permitted until 2024-04-25 16:32:34.309297131 +0000 UTC m=+808.510612940 (durationBeforeRetry 2m2s). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/2dabb154-16be-4272-bc08-641cf3d863ce-webhook-cert") pod "ingress-nginx-controller-84df5799c-8lrhc" (UID: "2dabb154-16be-4272-bc08-641cf3d863ce") : secret "ingress-nginx-admission" not found
Apr 25 16:30:33 minikube kubelet[1267]: I0425 16:30:33.072527    1267 scope.go:117] "RemoveContainer" containerID="a0ceba29cc317f1baa2ceb6d2562e833a3c346cedce9b93c12b0ac597aa853a5"
Apr 25 16:30:33 minikube kubelet[1267]: E0425 16:30:33.072976    1267 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"blog-web\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=blog-web pod=blog-web-66455df769-f6cs8_default(cbb95b54-642f-40cb-84bc-767af2de2b2f)\"" pod="default/blog-web-66455df769-f6cs8" podUID="cbb95b54-642f-40cb-84bc-767af2de2b2f"
Apr 25 16:30:44 minikube kubelet[1267]: I0425 16:30:44.071926    1267 scope.go:117] "RemoveContainer" containerID="a0ceba29cc317f1baa2ceb6d2562e833a3c346cedce9b93c12b0ac597aa853a5"
Apr 25 16:30:44 minikube kubelet[1267]: E0425 16:30:44.072640    1267 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"blog-web\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=blog-web pod=blog-web-66455df769-f6cs8_default(cbb95b54-642f-40cb-84bc-767af2de2b2f)\"" pod="default/blog-web-66455df769-f6cs8" podUID="cbb95b54-642f-40cb-84bc-767af2de2b2f"
Apr 25 16:30:56 minikube kubelet[1267]: I0425 16:30:56.074025    1267 scope.go:117] "RemoveContainer" containerID="a0ceba29cc317f1baa2ceb6d2562e833a3c346cedce9b93c12b0ac597aa853a5"
Apr 25 16:31:01 minikube kubelet[1267]: I0425 16:31:01.842396    1267 scope.go:117] "RemoveContainer" containerID="a0ceba29cc317f1baa2ceb6d2562e833a3c346cedce9b93c12b0ac597aa853a5"
Apr 25 16:31:01 minikube kubelet[1267]: I0425 16:31:01.843446    1267 scope.go:117] "RemoveContainer" containerID="40e8921bad24b1e571e372d68b76229506dbd3ed81e888ade3571f7455b67bd3"
Apr 25 16:31:01 minikube kubelet[1267]: E0425 16:31:01.844787    1267 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"blog-web\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=blog-web pod=blog-web-66455df769-f6cs8_default(cbb95b54-642f-40cb-84bc-767af2de2b2f)\"" pod="default/blog-web-66455df769-f6cs8" podUID="cbb95b54-642f-40cb-84bc-767af2de2b2f"
Apr 25 16:31:17 minikube kubelet[1267]: I0425 16:31:17.072523    1267 scope.go:117] "RemoveContainer" containerID="40e8921bad24b1e571e372d68b76229506dbd3ed81e888ade3571f7455b67bd3"
Apr 25 16:31:17 minikube kubelet[1267]: E0425 16:31:17.073087    1267 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"blog-web\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=blog-web pod=blog-web-66455df769-f6cs8_default(cbb95b54-642f-40cb-84bc-767af2de2b2f)\"" pod="default/blog-web-66455df769-f6cs8" podUID="cbb95b54-642f-40cb-84bc-767af2de2b2f"
Apr 25 16:31:30 minikube kubelet[1267]: I0425 16:31:30.073345    1267 scope.go:117] "RemoveContainer" containerID="40e8921bad24b1e571e372d68b76229506dbd3ed81e888ade3571f7455b67bd3"
Apr 25 16:31:30 minikube kubelet[1267]: E0425 16:31:30.075152    1267 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"blog-web\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=blog-web pod=blog-web-66455df769-f6cs8_default(cbb95b54-642f-40cb-84bc-767af2de2b2f)\"" pod="default/blog-web-66455df769-f6cs8" podUID="cbb95b54-642f-40cb-84bc-767af2de2b2f"
Apr 25 16:31:45 minikube kubelet[1267]: I0425 16:31:45.073016    1267 scope.go:117] "RemoveContainer" containerID="40e8921bad24b1e571e372d68b76229506dbd3ed81e888ade3571f7455b67bd3"
Apr 25 16:31:45 minikube kubelet[1267]: E0425 16:31:45.074436    1267 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"blog-web\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=blog-web pod=blog-web-66455df769-f6cs8_default(cbb95b54-642f-40cb-84bc-767af2de2b2f)\"" pod="default/blog-web-66455df769-f6cs8" podUID="cbb95b54-642f-40cb-84bc-767af2de2b2f"
Apr 25 16:31:57 minikube kubelet[1267]: I0425 16:31:57.072578    1267 scope.go:117] "RemoveContainer" containerID="40e8921bad24b1e571e372d68b76229506dbd3ed81e888ade3571f7455b67bd3"
Apr 25 16:31:57 minikube kubelet[1267]: E0425 16:31:57.073014    1267 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"blog-web\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=blog-web pod=blog-web-66455df769-f6cs8_default(cbb95b54-642f-40cb-84bc-767af2de2b2f)\"" pod="default/blog-web-66455df769-f6cs8" podUID="cbb95b54-642f-40cb-84bc-767af2de2b2f"
Apr 25 16:32:10 minikube kubelet[1267]: I0425 16:32:10.073708    1267 scope.go:117] "RemoveContainer" containerID="40e8921bad24b1e571e372d68b76229506dbd3ed81e888ade3571f7455b67bd3"
Apr 25 16:32:10 minikube kubelet[1267]: E0425 16:32:10.074971    1267 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"blog-web\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=blog-web pod=blog-web-66455df769-f6cs8_default(cbb95b54-642f-40cb-84bc-767af2de2b2f)\"" pod="default/blog-web-66455df769-f6cs8" podUID="cbb95b54-642f-40cb-84bc-767af2de2b2f"
Apr 25 16:32:15 minikube kubelet[1267]: I0425 16:32:15.157758    1267 reconciler_common.go:161] "operationExecutor.UnmountVolume started for volume \"kube-api-access-pvsj6\" (UniqueName: \"kubernetes.io/projected/4791153d-5929-40c2-ac83-83e81f9397fb-kube-api-access-pvsj6\") pod \"4791153d-5929-40c2-ac83-83e81f9397fb\" (UID: \"4791153d-5929-40c2-ac83-83e81f9397fb\") "
Apr 25 16:32:15 minikube kubelet[1267]: I0425 16:32:15.166517    1267 operation_generator.go:887] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/4791153d-5929-40c2-ac83-83e81f9397fb-kube-api-access-pvsj6" (OuterVolumeSpecName: "kube-api-access-pvsj6") pod "4791153d-5929-40c2-ac83-83e81f9397fb" (UID: "4791153d-5929-40c2-ac83-83e81f9397fb"). InnerVolumeSpecName "kube-api-access-pvsj6". PluginName "kubernetes.io/projected", VolumeGidValue ""
Apr 25 16:32:15 minikube kubelet[1267]: I0425 16:32:15.258748    1267 reconciler_common.go:289] "Volume detached for volume \"kube-api-access-pvsj6\" (UniqueName: \"kubernetes.io/projected/4791153d-5929-40c2-ac83-83e81f9397fb-kube-api-access-pvsj6\") on node \"minikube\" DevicePath \"\""
Apr 25 16:32:15 minikube kubelet[1267]: I0425 16:32:15.814584    1267 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="06241de7ec21f3b7b49debed7f6090b21c8272dde7db76f285daf13ef62c5306"
Apr 25 16:32:16 minikube kubelet[1267]: I0425 16:32:16.268050    1267 reconciler_common.go:161] "operationExecutor.UnmountVolume started for volume \"kube-api-access-jh2hw\" (UniqueName: \"kubernetes.io/projected/9c821b12-3c56-4991-bf5f-052725bee8a7-kube-api-access-jh2hw\") pod \"9c821b12-3c56-4991-bf5f-052725bee8a7\" (UID: \"9c821b12-3c56-4991-bf5f-052725bee8a7\") "
Apr 25 16:32:16 minikube kubelet[1267]: I0425 16:32:16.277179    1267 operation_generator.go:887] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/9c821b12-3c56-4991-bf5f-052725bee8a7-kube-api-access-jh2hw" (OuterVolumeSpecName: "kube-api-access-jh2hw") pod "9c821b12-3c56-4991-bf5f-052725bee8a7" (UID: "9c821b12-3c56-4991-bf5f-052725bee8a7"). InnerVolumeSpecName "kube-api-access-jh2hw". PluginName "kubernetes.io/projected", VolumeGidValue ""
Apr 25 16:32:16 minikube kubelet[1267]: I0425 16:32:16.369446    1267 reconciler_common.go:289] "Volume detached for volume \"kube-api-access-jh2hw\" (UniqueName: \"kubernetes.io/projected/9c821b12-3c56-4991-bf5f-052725bee8a7-kube-api-access-jh2hw\") on node \"minikube\" DevicePath \"\""
Apr 25 16:32:16 minikube kubelet[1267]: I0425 16:32:16.884272    1267 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="7fa9b67807af2297c652824c6c723f28a47b6ce343e7f575939cfe58e4baaf68"
Apr 25 16:32:21 minikube kubelet[1267]: I0425 16:32:21.072043    1267 scope.go:117] "RemoveContainer" containerID="40e8921bad24b1e571e372d68b76229506dbd3ed81e888ade3571f7455b67bd3"
Apr 25 16:32:21 minikube kubelet[1267]: E0425 16:32:21.072483    1267 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"blog-web\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=blog-web pod=blog-web-66455df769-f6cs8_default(cbb95b54-642f-40cb-84bc-767af2de2b2f)\"" pod="default/blog-web-66455df769-f6cs8" podUID="cbb95b54-642f-40cb-84bc-767af2de2b2f"
Apr 25 16:32:36 minikube kubelet[1267]: I0425 16:32:36.073662    1267 scope.go:117] "RemoveContainer" containerID="40e8921bad24b1e571e372d68b76229506dbd3ed81e888ade3571f7455b67bd3"
Apr 25 16:32:36 minikube kubelet[1267]: E0425 16:32:36.074544    1267 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"blog-web\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=blog-web pod=blog-web-66455df769-f6cs8_default(cbb95b54-642f-40cb-84bc-767af2de2b2f)\"" pod="default/blog-web-66455df769-f6cs8" podUID="cbb95b54-642f-40cb-84bc-767af2de2b2f"
Apr 25 16:32:47 minikube kubelet[1267]: I0425 16:32:47.074564    1267 scope.go:117] "RemoveContainer" containerID="40e8921bad24b1e571e372d68b76229506dbd3ed81e888ade3571f7455b67bd3"
Apr 25 16:32:47 minikube kubelet[1267]: E0425 16:32:47.075379    1267 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"blog-web\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=blog-web pod=blog-web-66455df769-f6cs8_default(cbb95b54-642f-40cb-84bc-767af2de2b2f)\"" pod="default/blog-web-66455df769-f6cs8" podUID="cbb95b54-642f-40cb-84bc-767af2de2b2f"
Apr 25 16:33:00 minikube kubelet[1267]: I0425 16:33:00.071981    1267 scope.go:117] "RemoveContainer" containerID="40e8921bad24b1e571e372d68b76229506dbd3ed81e888ade3571f7455b67bd3"
Apr 25 16:33:00 minikube kubelet[1267]: E0425 16:33:00.072271    1267 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"blog-web\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=blog-web pod=blog-web-66455df769-f6cs8_default(cbb95b54-642f-40cb-84bc-767af2de2b2f)\"" pod="default/blog-web-66455df769-f6cs8" podUID="cbb95b54-642f-40cb-84bc-767af2de2b2f"
Apr 25 16:33:12 minikube kubelet[1267]: I0425 16:33:12.072694    1267 scope.go:117] "RemoveContainer" containerID="40e8921bad24b1e571e372d68b76229506dbd3ed81e888ade3571f7455b67bd3"
Apr 25 16:33:12 minikube kubelet[1267]: E0425 16:33:12.073682    1267 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"blog-web\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=blog-web pod=blog-web-66455df769-f6cs8_default(cbb95b54-642f-40cb-84bc-767af2de2b2f)\"" pod="default/blog-web-66455df769-f6cs8" podUID="cbb95b54-642f-40cb-84bc-767af2de2b2f"
Apr 25 16:33:24 minikube kubelet[1267]: I0425 16:33:24.071998    1267 scope.go:117] "RemoveContainer" containerID="40e8921bad24b1e571e372d68b76229506dbd3ed81e888ade3571f7455b67bd3"
Apr 25 16:33:24 minikube kubelet[1267]: E0425 16:33:24.072527    1267 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"blog-web\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=blog-web pod=blog-web-66455df769-f6cs8_default(cbb95b54-642f-40cb-84bc-767af2de2b2f)\"" pod="default/blog-web-66455df769-f6cs8" podUID="cbb95b54-642f-40cb-84bc-767af2de2b2f"
Apr 25 16:33:35 minikube kubelet[1267]: I0425 16:33:35.071728    1267 scope.go:117] "RemoveContainer" containerID="40e8921bad24b1e571e372d68b76229506dbd3ed81e888ade3571f7455b67bd3"
Apr 25 16:33:35 minikube kubelet[1267]: E0425 16:33:35.072179    1267 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"blog-web\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=blog-web pod=blog-web-66455df769-f6cs8_default(cbb95b54-642f-40cb-84bc-767af2de2b2f)\"" pod="default/blog-web-66455df769-f6cs8" podUID="cbb95b54-642f-40cb-84bc-767af2de2b2f"
Apr 25 16:33:50 minikube kubelet[1267]: I0425 16:33:50.072118    1267 scope.go:117] "RemoveContainer" containerID="40e8921bad24b1e571e372d68b76229506dbd3ed81e888ade3571f7455b67bd3"
Apr 25 16:33:50 minikube kubelet[1267]: E0425 16:33:50.072547    1267 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"blog-web\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=blog-web pod=blog-web-66455df769-f6cs8_default(cbb95b54-642f-40cb-84bc-767af2de2b2f)\"" pod="default/blog-web-66455df769-f6cs8" podUID="cbb95b54-642f-40cb-84bc-767af2de2b2f"
Apr 25 16:34:01 minikube kubelet[1267]: I0425 16:34:01.072179    1267 scope.go:117] "RemoveContainer" containerID="40e8921bad24b1e571e372d68b76229506dbd3ed81e888ade3571f7455b67bd3"
Apr 25 16:34:01 minikube kubelet[1267]: E0425 16:34:01.073037    1267 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"blog-web\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=blog-web pod=blog-web-66455df769-f6cs8_default(cbb95b54-642f-40cb-84bc-767af2de2b2f)\"" pod="default/blog-web-66455df769-f6cs8" podUID="cbb95b54-642f-40cb-84bc-767af2de2b2f"
Apr 25 16:34:12 minikube kubelet[1267]: I0425 16:34:12.072620    1267 scope.go:117] "RemoveContainer" containerID="40e8921bad24b1e571e372d68b76229506dbd3ed81e888ade3571f7455b67bd3"
Apr 25 16:34:12 minikube kubelet[1267]: E0425 16:34:12.074212    1267 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"blog-web\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=blog-web pod=blog-web-66455df769-f6cs8_default(cbb95b54-642f-40cb-84bc-767af2de2b2f)\"" pod="default/blog-web-66455df769-f6cs8" podUID="cbb95b54-642f-40cb-84bc-767af2de2b2f"
Apr 25 16:34:24 minikube kubelet[1267]: I0425 16:34:24.074012    1267 scope.go:117] "RemoveContainer" containerID="40e8921bad24b1e571e372d68b76229506dbd3ed81e888ade3571f7455b67bd3"
Apr 25 16:34:24 minikube kubelet[1267]: E0425 16:34:24.076607    1267 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"blog-web\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=blog-web pod=blog-web-66455df769-f6cs8_default(cbb95b54-642f-40cb-84bc-767af2de2b2f)\"" pod="default/blog-web-66455df769-f6cs8" podUID="cbb95b54-642f-40cb-84bc-767af2de2b2f"
Apr 25 16:34:39 minikube kubelet[1267]: I0425 16:34:39.072029    1267 scope.go:117] "RemoveContainer" containerID="40e8921bad24b1e571e372d68b76229506dbd3ed81e888ade3571f7455b67bd3"
Apr 25 16:34:39 minikube kubelet[1267]: E0425 16:34:39.072454    1267 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"blog-web\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=blog-web pod=blog-web-66455df769-f6cs8_default(cbb95b54-642f-40cb-84bc-767af2de2b2f)\"" pod="default/blog-web-66455df769-f6cs8" podUID="cbb95b54-642f-40cb-84bc-767af2de2b2f"
Apr 25 16:34:54 minikube kubelet[1267]: I0425 16:34:54.071652    1267 scope.go:117] "RemoveContainer" containerID="40e8921bad24b1e571e372d68b76229506dbd3ed81e888ade3571f7455b67bd3"
Apr 25 16:34:54 minikube kubelet[1267]: E0425 16:34:54.072108    1267 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"blog-web\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=blog-web pod=blog-web-66455df769-f6cs8_default(cbb95b54-642f-40cb-84bc-767af2de2b2f)\"" pod="default/blog-web-66455df769-f6cs8" podUID="cbb95b54-642f-40cb-84bc-767af2de2b2f"


==> storage-provisioner [2fea2f428033] <==
I0425 16:19:11.807503       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0425 16:19:42.503548       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: i/o timeout


==> storage-provisioner [e75deb31ebfe] <==
I0425 16:19:58.232757       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0425 16:19:58.243847       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0425 16:19:58.244067       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0425 16:20:15.655228       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0425 16:20:15.655497       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_cb78bbb8-18bd-4e86-ad54-d5875d92c8c9!
I0425 16:20:15.659343       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"97ac843a-99de-45f0-ab1f-af3c2c7ff9dc", APIVersion:"v1", ResourceVersion:"4619", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_cb78bbb8-18bd-4e86-ad54-d5875d92c8c9 became leader
I0425 16:20:15.760839       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_cb78bbb8-18bd-4e86-ad54-d5875d92c8c9!

